{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_design_experiments_analysis_covariance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0qqeemIaxihp850PmEnJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/design_of_experiments/blob/master/6_design_experiments_analysis_covariance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQkA8AlJrpnt",
        "colab_type": "text"
      },
      "source": [
        "#6 The analysis of covariance "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgXvigS7rpqO",
        "colab_type": "text"
      },
      "source": [
        "In Chapters 2 and 4, we introduced the use of the blocking principle to improve the precision with which comparisons between treatments are made. The paired t-test was the procedure illustrated in Chapter 2, and the randomized block design was presented in Chapter 4.\n",
        "\n",
        "In general, the blocking principle can be used to eliminate the effect of controllable nuisance factors. The **analysis of covariance (ANCOVA)** is another technique that is occasionally useful for improving the precision of an experiment. Suppose that in an experiment with a response variable y there is another variable, say x, and that y is linearly related to x. Furthermore, suppose that x cannot be controlled by the experimenter but can be observed\n",
        "along with y. The variable x is called a **covariate**. The analysis of\n",
        "covariance involves adjusting the observed response variable for the effect of the covariate. If such an adjustment is not performed, the covariate could inflate the error mean square and make true differences in the response due to treatments harder to detect. Thus, the analysis of covariance is a method of adjusting for the effects of an uncontrollable nuisance variable. As we will see, the procedure is a combination of analysis of variance and regression analysis.\n",
        "\n",
        "**Example:** A study performed to determine if there is a difference in the strength of a monofilament fiber produced by three different machines. The data from this experiment are shown in Table 15.10.\n",
        "\n",
        "![alt text](https://i.ibb.co/HgSqKzV/breaking-strength-data.png)\n",
        "\n",
        "Figure 15.3 presents a scatter diagram of strength (y) versus the diameter\n",
        "(or thickness) of the sample.\n",
        "\n",
        "![alt text](https://i.ibb.co/jZDqJJT/strength-vs-diameter-plot.png)\n",
        "\n",
        "The strength of the fiber is also affected by its thickness; consequently, a thicker fiber will generally be stronger than a thinner one. \n",
        "\n",
        "The analysis of covariance could be used to remove the effect of the thickness (x) on strength ($y$) when testing for differences in strength between machines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CptugUNgJkW",
        "colab_type": "text"
      },
      "source": [
        "## 8.1 Description of the procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ai86Eabrpsy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The basic procedure for the analysis of covariance is now described and illustrated for a single-factor experiment with one covariate. Assuming that there is a linear relationship between the response and the covariate, we find that an appropriate statistical model is\n",
        "\n",
        "$$y_{ij} = \\mu+\\tau_i + \\beta(x_{ij}-\\overline{x}_{..}) + \\epsilon_{ij}; i=1,2,...,a; j=1,...,n$$ \n",
        "\n",
        "where $y_{ij}$ is the $j$th observation on the response variable taken under the $i$th treatment or level of the single factor, $x_{ij}$ is the measurement made on the covariate corresponding to $y_{ij}$ (i.e., the $ij$th run), $\\overline{x}_{..}$ is the mean of the $x_{ij}$ values, $\\mu$ is an overall mean, $\\mu_i$ is the effect of the $i$th treatment, $\\beta$ is a linear regression coefficient indicating the dependency of $y_{ij}$ on $x_{ij}$ , and  $\\epsilon_{ij}$ is a random error component. We assume that the errors $\\epsilon_{ij}$ are NID(0, $\\sigma^2$), that the slope $\\beta \\neq 0$ and the true relationship between $y_{ij}$ on $x_{ij}$  is linear, that the regression\n",
        "coefficients for each treatment are identical, that the treatment effects sum to zero ($\\sum_{i=1}^{a}\\tau_i=0$), and that the covariate $x_{ij}$ is not affected by the treatments.\n",
        "\n",
        "This model assumes that all treatment regression lines have identical slopes. If the treatments interact with the covariates this can result in non-identical slopes. Covariance analysis is not appropriate in these cases. Estimating and comparing different regression models is the correct approach.\n",
        "\n",
        "The equation of the model assumes a linear relationship between $y$ and $x$. However, any other relationship, such as a quadratic (for example) could be used.\n",
        "\n",
        "Notice from the equation that the analysis of the covariance model is a combination of the linear models employed in the analysis of variance and regression. That is, we have treatment effects {$\\tau_i$} as in a single-factor analysis of variance and a regression coefficient $\\beta$ as in a regression equation. The covariate in Equation is expressed as ($x_{ij}-\\overline{x}_{..}$) instead of $x_{ij}$ so that the parameter $\\mu$ is preserved as the overall mean.\n",
        "\n",
        "To describe the analysis, we introduce the following notation:\n",
        "\n",
        "$$SS_{yy}=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(y_{ij}-\\overline{y}_{..})^2=\\sum_{i=1}^{a}\\sum_{j=1}^{n}y_{ij}^2-\\frac{y_{..}^2}{an}$$\n",
        "\n",
        "$$SS_{xx}=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(x_{ij}-\\overline{x}_{..})^2=\\sum_{i=1}^{a}\\sum_{j=1}^{n}x_{ij}^2-\\frac{x_{..}^2}{an}$$\n",
        "\n",
        "$$SS_{xy}=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(y_{ij}-\\overline{y}_{..})(x_{ij}-\\overline{x}_{..})=\\sum_{i=1}^{a}\\sum_{j=1}^{n}y_{ij}x_{ij}-\\frac{y_{..}x_{..}}{an}$$\n",
        "\n",
        "$$TT_{yy}=n\\sum_{i=1}^{a}(\\overline{y}_{i.}-\\overline{y}_{..})^2=\\frac{1}{n}\\sum_{i=1}^{a}y_{i.}^2-\\frac{y_{..}^2}{an}$$\n",
        "\n",
        "$$TT_{xx}=n\\sum_{i=1}^{a}(\\overline{x}_{i.}-\\overline{x}_{..})^2=\\frac{1}{n}\\sum_{i=1}^{a}x_{i.}^2-\\frac{x_{..}^2}{an}$$\n",
        "\n",
        "$$TT_{xy}=n\\sum_{i=1}^{a}(\\overline{y}_{i.}-\\overline{y}_{..})(\\overline{x}_{i.}-\\overline{x}_{..})=\\frac{1}{n}\\sum_{i=1}^{a}y_{i.}x_{i.}-\\frac{y_{..}x_{..}}{an}$$\n",
        "\n",
        "$$E_{yy}=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(y_{ij}-\\overline{y}_{..})^2=S_{yy}-T_{yy}$$\n",
        "\n",
        "$$E_{xx}=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(x_{ij}-\\overline{x}_{..})^2=S_{xx}-T_{xx}$$\n",
        "\n",
        "$$E_{xy}=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(y_{ij}-\\overline{y}_{i.})(x_{ij}-\\overline{x}_{i.})=S_{xy}-T_{xy}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0VQWlh1rpvg",
        "colab_type": "text"
      },
      "source": [
        "Note that, in general, $S= T + E$, where the symbols $S$, $T$, and $E$ are used to denote sums of squares and cross products for total, treatments, and error, respectively. The sums of squares for $x$ and $y$ must be nonnegative; however, the sums of cross products ($xy$) may be negative.\n",
        "\n",
        "We now show how the analysis of covariance adjusts the response variable for the effect of the covariate. Consider the full model. The least squares estimators of $\\mu$, $\\tau_i$, and $\\beta$ are $$\\hat{\\mu}=\\overline{y}_{..}$$, $$\\hat{\\tau}_{i}=\\overline{y}_{i.}-\\overline{y}_{..}$$ and\n",
        "$$\\hat{\\beta}=\\frac{E_{xy}}{E_{xx}}$$\n",
        "\n",
        "The error sum of squares in this model is\n",
        "\n",
        "$$SS_E=E_{yy} - \\frac{E_{xy}^2}{E_{xx}}$$\n",
        "\n",
        "with $a(n-1)-1$ degrees of freedom. The experimental error variance is estimated by\n",
        "\n",
        "$$MS_E=\\frac{SS_E}{a(n-1)-1}$$\n",
        "\n",
        "Now suppose that there is no treatment effect. The model would then be\n",
        "\n",
        "$$y_{ij} = \\mu + \\beta(x_{ij}-\\overline{x}_{..}) + \\epsilon_{ij}$$\n",
        "\n",
        "and it can be shown that the least squares estimators of $\\mu$ and $\\beta$ are $\\hat{\\mu}=\\overline{y}_{..}$ and $\\hat{\\beta}=\\frac{S_{xy}}{S_{xx}}$.\n",
        "The sum of squares for error in this reduced model is\n",
        "$$SS_E^{\\prime}=S_{yy} - \\frac{S_{xy}^2}{S_{xx}}$$\n",
        "\n",
        "with $an-2$ degrees of freedom. In this last equation, the quantity $\\frac{S_{xy}^2}{S_{xx}}$ is the reduction in the sum of squares of $y$ obtained through the linear regression of $y$ on $x$. Furthermore, note\n",
        "that $SS_E$ is smaller than $SS_E^{\\prime}$ [because the full model contains additional parameters {$\\tau_i$}] and that the quantity $SS_E^{\\prime}-SS_E$ is a reduction in the sum of squares due to the {$\\tau_i$}.\n",
        "Therefore, the difference between $SS_E^{\\prime}$ and $SS_E$ , that is, $SS_E^{\\prime}-SS_E$, provides a sum of squares with $a-1$ degrees of freedom for testing the hypothesis of no treatment effects.\n",
        "\n",
        "Consequently, to test $H_0:\\tau_i=0$, compute\n",
        "\n",
        "$$F_0=\\frac{\\frac{SS_E^{\\prime}-SS_E}{a-1}}{\\frac{SS_E}{a(n-1)-1}}$$\n",
        "\n",
        "which, if the null hypothesis is true, is distributed as $F_{\\alpha,a-1,a(n-1)1}$. Thus, we reject $H_0$ if $F_0>F_{\\alpha,a-1,a(n-1)1}$ . The P-value approach could also be used.\n",
        "\n",
        "It is instructive to examine the display in Table 15.11.\n",
        "\n",
        "![alt text](https://i.ibb.co/wQ3jmRF/table-15-1.png)\n",
        "\n",
        "In this table, we have presented the analysis of covariance as an “adjusted” analysis of variance. In the source of variation column, the total variability is measured by $S_{yy}$ with $an-1$ degrees of freedom. The source of variation “regression” has the sum of squares $\\frac{S_{xy}^2}{S_{xx}}$ with one degree of freedom. If there were no covariate, we would have $S_{xy}=S_{xx}=E_{xy}=E_{xx}=0$. Then the sum of squares for error would be simply $E_{yy}$ and the sum of squares for treatments would be $S_{yy}-E_{yy}=T_{yy}$ . However, because of the presence of the covariate, we must “adjust” $S_{yy}$ and $E_{yy}$ for the regression of $y$ on $x$ as shown in Table 15.11. The adjusted error sum of squares has $a(n-1)-1$ degrees of freedom instead of $a(n-1$) degrees of freedom because an additional parameter (the slope $\\beta$) is fitted to the data.\n",
        "\n",
        "Manual computations are usually displayed in an analysis of covariance table such as Table 15.12.\n",
        "\n",
        "![alt text](https://i.ibb.co/tLfTLwc/table-15-12.png)\n",
        "\n",
        "This layout is employed because it conveniently summarizes all the required\n",
        "sums of squares and cross products as well as the sums of squares for testing hypotheses about treatment effects.\n",
        "\n",
        "\n",
        "In addition to testing the hypothesis that there are no differences in the treatment effects, we frequently find it useful in interpreting the data to present the adjusted treatment means. These adjusted means are computed according to\n",
        "\n",
        "$$\\text{Adjusted }\\overline{y}_{i.}=\\overline{y}_{i.}-\\hat{\\beta}(\\overline{x}_{i.}-\\overline{x}_{..})$$\n",
        "\n",
        "where $\\hat{\\beta}=\\frac{E_{xy}}{E_{xx}}$. This adjusted treatment mean is the least squares estimator of $\\mu+\\tau_i, i=1,...a$ in the full model. The standard error of any adjusted treatment mean is\n",
        "\n",
        "$$S_{\\text{Adj}\\overline{y}_{i.}}=\\sqrt{MS_E(\\frac{1}{n}+\\frac{(\\overline{x}_{i.}-\\overline{x}_{..})^2}{E_{xx}})}$$\n",
        "\n",
        "Finally, we recall that the regression coefficient $\\beta$ in the full model has been assumed to be nonzero. We may test the hypothesis $H_0:\\beta=0$ by using the test statistic\n",
        "\n",
        "$$F_0=\\frac{\\frac{E_{xy}^2}{E_{xx}}}{MS_E}$$ which under the null hypothesis is distributed as $F_{\\alpha,1,a(n-1)-1}$ . Thus, we reject $H_0$ if $F_0>F_{\\alpha,1,a(n-1)-1}$.\n",
        "\n",
        "**Example:** Consider the experiment described at the beginning of the section. In the next table, we show the results of the test.\n",
        "\n",
        "![alt text](https://i.ibb.co/jHRGgCF/table-15-13.png)\n",
        "\n",
        "The value of the statistic is $F_0=2.03<F_{0.10,2,12}$ (the P-value is smaller than the level $\\alpha$), so there is no reason to believe that machines produce fibers of different diameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfKTWtsogiRz",
        "colab_type": "text"
      },
      "source": [
        "## 8.2 Checking assumptions of the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW0MYLr6rpyH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The diagnostic checking of the covariance model is based on the residual analysis. For the covariance model, the residuals are\n",
        "$$e_{ij}=y_{ij}-\\hat{y}_{ij}$$\n",
        "\n",
        "The residuals are plotted versus the fitted values $\\hat{y}_{ij}$ in Figure 15.4, versus the covariate $x_{ij}$ in Figure 15.5 and versus the machines in Figure 15.6. A normal probability plot of the residuals is shown in Figure 15.7.\n",
        "\n",
        "![alt text](https://i.ibb.co/30xJkWy/figure-15-4-5-png.png)\n",
        "\n",
        "![alt text](https://i.ibb.co/54BxmwQ/figure-15-6-7.png)\n",
        "\n",
        "\n",
        "These plots do not reveal any major departures from the assumptions, so we conclude that the covariance model is appropriate for the breaking strength data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7fE7Hxorp05",
        "colab_type": "text"
      },
      "source": [
        "**Observation**: It is interesting to note what would have happened in this experiment if an analysis of covariance had not been performed, that is, if the breaking strength data ($y$) had been analyzed as a completely randomized single-factor experiment in which the covariate $x$ was ignored.\n",
        "The analysis of variance of the breaking strength data is shown in Table 15.14.\n",
        "\n",
        "![alt text](https://i.ibb.co/qmpz2Fh/table-15-14.png)\n",
        "\n",
        "\n",
        "We immediately notice that the error estimate is much longer in the CRD analysis (17.17 versus 2.54). This is a reflection of the effectiveness of the analysis of covariance in reducing error variability. We would also conclude, based on the CRD analysis, that machines differ significantly in the strength of fiber produced. This is exactly opposite the conclusion reached by the covariance analysis. If we suspected that the machines differed significantly in their effect on fiber strength, then we would try to equalize the strength output of the three machines. However, in this problem the machines do not differ in the strength of fiber produced after the linear effect of fiber diameter is removed. It would be helpful to reduce the within-machine fiber\n",
        "diameter variability because this would probably reduce the strength variability in the fiber.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX5LKpmDrp50",
        "colab_type": "text"
      },
      "source": [
        "**Analysis of covariance as an alternative to blocking**: In some situations, the experimenter may have a choice between either running a completely randomized design with a covariate or running a randomized block design with the covariate used in some fashion to form the blocks. If the relationship between the covariate and the response is well-approximated by a straight line and that is the form of the covariance model that the\n",
        "experimenter chooses, then either of these approaches is about equally effective. However, if the relationship isn’t linear and a linear model is assumed, the analysis of covariance will be outperformed in error reduction by the randomized block design. Randomized block designs do not make any explicit assumptions about relationships between the nuisance variables (covariates) and the response. Generally, a randomized block design will have fewer\n",
        "degrees of freedom for error than a covariance model but the resulting loss of statistical power is usually quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgzl9ClugwsR",
        "colab_type": "text"
      },
      "source": [
        "## 8.3 Factorial experiments with covariates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKYQO5qlVPPs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Analysis of covariance can be applied to more complex treatment structures, such as factorial designs. Provided enough data exists for every treatment combination, nearly any complex treatment structure can be analyzed through the analysis of covariance approach. We now show how the analysis of covariance could be used in the most common family of factorial designs used in industrial experimentation, the $2^k$ factorials.\n",
        "\n",
        "Imposing the assumption that the covariate affects the response variable identically across all treatment combinations, an analysis of covariance table similar to the procedure given in section 8.1 could be performed. The only difference would be the treatment sum of squares. For a $2^2$ factorial with $n$ replicates, the treatment sum of squares ($T_{yy}$) would be\n",
        "$TT_{yy}=\\frac{1}{n}\\sum_{i=1}^{2}\\sum_{i=1}^{2}y_{ij.}^2-\\frac{y_{...}^2}{(2)(2)n}$ . This quantity is the sum of the sums of squares for factors A,\n",
        "B, and the AB interaction. The adjusted treatment sum of squares could then be partitioned into individual effect components, that is, adjusted main effects sum of squares $SS_A$ and $SS_B$, and an adjusted interaction sum of squares, $SS_{AB}$.\n",
        "\n",
        "**The amount of replication**: Consider a $2^3$ factorial arrangement. A minimum of two replicates is needed to evaluate all treatment combinations with a separate covariate for each treatment combination\n",
        "(covariate by treatment interaction). This is equivalent to fitting a simple regression model to each treatment combination or design cell. With two observations per cell, one degree of freedom is used to estimate the intercept (the treatment effect), and the other is used to estimate the slope (the covariate effect). With this *saturated* model, no degrees of freedom are available to estimate the error. Thus, at least three replicates are needed for a complete analysis of\n",
        "covariance, assuming the most general case. This problem becomes more pronounced as the number of distinct design cells (treatment combinations) and covariates increases. If the amount of replication is limited, various assumptions can be made to allow some useful analysis.\n",
        "- The simplest assumption (and typically the worst) that can be made is that the covariate has no effect. If the covariate is erroneously not considered, the entire analysis and subsequent conclusions could be dramatically in error.\n",
        "- Another choice is to assume that there is no treatment by covariate interaction. Even if this assumption is incorrect, the average\n",
        "effect of the covariate across all treatments will still increase the precision of the estimation and testing of the treatment effects. One disadvantage of this assumption is that if several treatment levels interact with the covariate, the various terms may cancel one another out and the covariate term if estimated alone with no interaction, may be insignificant.\n",
        "- A third choice would be to assume some of the factors (such as some two-factor and higher interactions) are insignificant. This allows some degrees of freedom to be used to estimate error. This course\n",
        "of action, however, should be undertaken carefully and the subsequent models evaluated thoroughly because the estimation of error will be relatively imprecise unless enough degrees of freedom are allocated for it."
      ]
    }
  ]
}