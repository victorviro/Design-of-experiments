{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_design_of_experiments_randomized_blocks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNefGvFWJlxE0ulbTVgbiDN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/design_of_experiments/blob/master/4_design_of_experiments_randomized_blocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G211OgwkJ_x",
        "colab_type": "text"
      },
      "source": [
        "# 4. Randomized Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD8_xufnkKC4",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 The randomized complete block design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWIWutaskaPE",
        "colab_type": "text"
      },
      "source": [
        "In any experiment, variability arising from a nuisance factor can affect the results. Generally, we define a nuisance factor as a design factor that probably has an effect on the response, but we are not interested in that effect. When the nuisance source of variability is **known and controllable**, a design technique called **blocking** can be used to systematically eliminate its effect on the statistical comparisons among treatments. Blocking is an extremely important design technique used extensively in industrial experimentation and is the subject of this chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtF5YXtOomFp",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.1 Statistical analysis of the randomized complete block design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEAAIlb8omIP",
        "colab_type": "text"
      },
      "source": [
        "Suppose we have, in general, $a$ treatments that are to be compared and $b$ blocks. The randomized complete block design (RCBD) is shown in Figure 4.1. There is one observation per treatment in each block, and the order in which the treatments are run within each block is determined randomly. Because the only randomization of treatments is within the blocks, we often say that the blocks represent a restriction on randomization.\n",
        "\n",
        "![alt text](https://i.ibb.co/Lg29YWx/rcbd.png)\n",
        "\n",
        "The **statistical model** for the RCBD can be written in several ways. The  traditional model is an **effects model**:\n",
        "\n",
        "$$y_{ij} = \\mu+\\tau_i + \\beta_j +\\epsilon_{ij}; i=1,2,...,a; j=1,...,b$$ \n",
        "\n",
        "where $\\mu$ is the overall mean, and $\\tau_i$ is the $i$th treatment called the $i$th treatment effect, $\\beta_j$ is the effect of the $j$th block, and $\\epsilon_{ij}$ is the usual NID(0, $\\sigma^2$) random error term. \n",
        "\n",
        "We will initially consider treatments and blocks to be fixed factors. The case of random blocks, which is very important, is considered in Section 4.1.3. Just as in the single-factor experimental design model in Chapter 3, the effects model for the RCBD is an overspecified model. Consequently, we usually think of the treatment and block effects as deviations from the overall mean so that\n",
        "\n",
        "$$\\sum_{i=1}^{a}\\tau_i=0 \\text{ and } \\sum_{i=1}^{b}\\beta_j=0$$\n",
        "\n",
        "In an experiment involving the RCBD, we are interested in testing the equality of the treatment means. Thus, the hypotheses of interest are\n",
        "\n",
        "$$\n",
        "H_0: \\mu_1=\\mu_2=...=\\mu_a\\\\\n",
        "H_1: \\mu_i\\neq\\mu_j \\text{ for at least one pair}(i,j)\n",
        "$$\n",
        "\n",
        "Because the $i$th treatment mean $mu_i=\\mu+\\tau_i$ , an equivalent way to write the above hypotheses is in terms of the treatment effects is\n",
        "$$\n",
        "H_0: \\tau_1=\\tau_2=...=\\tau_a=0\\\\\n",
        "H_1: \\tau_i\\neq0 \\text{ for  at  least  one }i\n",
        "$$\n",
        "\n",
        "The analysis of variance can be easily extended to the RCBD. Let $y_{i.}$ be the total of all observations taken under treatment $i$ ,$y_{.j}$ be the total of all observations in block $j$, $y_{..}$ be the grand total of all observations, and $N=\u0005ab$ be the total number of observations. Expressed mathematically,\n",
        "$$y_{i.}=\\sum_{j=1}^{b}y_{ij}$$  $$\\overline{y}_{i.}=\\frac{y_{i.}}{b}, i=1,...,a$$\n",
        "\n",
        "$$y_{.j}=\\sum_{i=1}^{a}y_{ij}$$  $$\\overline{y}_{.j}=\\frac{y_{.j}}{a}, i=1,...,a$$\n",
        "\n",
        "$$y_{..}=\\sum_{i=1}^{a}\\sum_{j=1}^{b}y_{ij}$$ $$\\overline{y}_{..}=\\frac{y_{..}}{N}$$\n",
        "\n",
        "where,$\\overline{y}_{i.}$ is the average of the observations taken under treatment $i$, $\\overline{y}_{.j}$ is the average of the observations in block $j$, and $\\overline{y}_{..}$ is the grand average of all observations.\n",
        "\n",
        "We may express the total corrected sum of squares as\n",
        "\n",
        "$$SS_T=\\sum_{i=1}^{a}\\sum_{j=1}^{b}(y_{ij}-\\overline{y}_{..})^2= b\\sum_{i=1}^{a}(\\overline{y}_{i.}-\\overline{y}_{..})^2 + a\\sum_{j=1}^{b}(\\overline{y}_{.j}-\\overline{y}_{..})^2 + \\sum_{i=1}^{a}\\sum_{j=1}^{b}(y_{ij}-\\overline{y}_{i.}-\\overline{y}_{.j}+\\overline{y}_{..})^2$$\n",
        "\n",
        "which represents a partition of the total sum of squares. This is the fundamental ANOVA equation for the RCBD. Expressing the sums of squares in this equation symbolically, we have\n",
        "$$\n",
        "SS_T = SS_{Treatments} + SS_{Blocks} + SS_E\n",
        "$$\n",
        "\n",
        "Because there are $N$ observations,$SS_T $ has $N-\u00021$ degrees of freedom. There are $a$ treatments and $b$ blocks, so $SS_{Treatments}$ and $SS_{Blocks}$ have $a-\u00021$ and $b-\u00021$ degrees of freedom, respectively. The error sum of squares is just a sum of squares between cells minus the sum of squares for treatments and blocks. There are $ab$ cells with $ab-\u00021$ degrees of freedom between them, so $SS_E$ has $ab-\u00021-\u0002(a-\u00021)-\u0002(b-\u00021)=\u0005(a-\u00021)(b-\u00021)$ degrees of freedom.  \n",
        "\n",
        "Furthermore, making the usual normality assumptions on the errors we can prove that  $\\frac{SS_{Treatments}}{\\sigma^2}$, $\\frac{SS_{Blocks}}{\\sigma^2}$, and $\\frac{SS_{E}}{\\sigma^2}$ are independently distributed chi-square random variables. Each sum of squares divided by its degrees of freedom is a mean square. The expected value of the mean squares, if treatments and blocks are fixed, are\n",
        "$$E(MS_{E})=\\sigma^2$$\n",
        "$$E(MS_{Treatments})=\\sigma^2+\\frac{b\\sum_{i=1}^{a}\\tau_i^2}{a-1}$$\n",
        "$$E(MS_{Blocks})=\\sigma^2+\\frac{a\\sum_{j=1}^{b}\\beta_j^2}{b-1}$$\n",
        "\n",
        "Therefore, to test the equality of treatment means, we would use the test statistic\n",
        "\n",
        "$$\n",
        "F_0=\\frac{\\frac{SS_{Treatments}}{a-1}}{\\frac{SS_E}{(a-1)(b-1)}}=\\frac{MS_{Treatments}}{MS_E}\n",
        "$$\n",
        "$F_0\\sim F_{a-1,(a-1)(b-1)}$ if the null hypothesis is true. The critical region is the upper tail of the $F$ distribution, and we would reject $H0$ if \n",
        "$$F_0>F_{\\alpha,a-1,(a-1)(b-1)}$$\n",
        "A P-value approach can also be used.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g1LsjdnPrUy",
        "colab_type": "text"
      },
      "source": [
        "We may also be interested in comparing block means because, if these means do not differ greatly, blocking may not be necessary for future experiments. From the expected mean squares, it seems that the hypothesis $H_0: \\beta_\u0003j\u0005=0$ may  be tested by comparing the statistic $F_0=\\frac{\u0005MS_{Blocks}}{MS_E}$ to $F_{\\alpha,b-1,(a-1)(b-1)}$. However, recall that randomization has been applied only to treatments within blocks; that is, the blocks represent a restriction on randomization. What effect does this have on the statistic $F_0=\\frac{\u0005MS_{Blocks}}{MS_E}$? Some differences regarding this question exist. For example, Box, Hunter, and Hunter (2005) point out that the usual analysis of variance $F$ test can be justified based on randomization only, without direct use of the normality assumption. They further observe that the test to compare block means cannot appeal to such a justification because of the randomization restriction, but if the errors are NID(0,\u0002$\\sigma^2$), the statistic $F_0=\\frac{\u0005MS_{Blocks}}{MS_E}$ can be used to compare block means.On the other hand, Anderson and McLean (1974) argue that the randomization restriction prevents this statistic from being a meaningful test for comparing block means and that this $F$ ratio is a test for the equality of the block means plus the randomization restriction.\n",
        "\n",
        "In practice, then, what do we do? Because the normality assumption is often questionable, to view $F_0=\\frac{\u0005MS_{Blocks}}{MS_E}$ as an exact $F$ test on the equality of block means is not a good general practice. For that reason, we exclude this $F$ test from the analysis of variance table. However, as an approximate procedure to investigate the effect of the blocking variable, examining the ratio of $MS_{Blocks}$ to $MS_E$ is certainly reasonable. If this ratio is large, it impliest hat the blocking factor has a large effect and that the noise reduction obtained by blocking was probably helpful in improving the precision of the comparison of treatment means. The procedure is usually summarized in an ANOVA table, such as the one shown in Table 4.2. The computing would usually be done with a statistical software package.\n",
        "\n",
        "![alt text](https://i.ibb.co/DzH984n/anova-rcbd.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEHa5ZtMPrYp",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, computing formulas can be expressed in terms of treatment and block totals. These formulas are\n",
        "$$$$\n",
        "\n",
        "The residuals are\n",
        "$$e_{ij}=y_{ij}-\\hat{y}_{ij}$$\n",
        "\n",
        "and, as we will later show, the fitted values are\n",
        "$$\\hat{y}_{ij}=\\overline{y}_{i.}+\\overline{y}_{.j}-\\overline{y}_{..}$$\n",
        "\n",
        "so\n",
        "$$e_{ij}=y_{ij}-\\overline{y}_{i.}-\\overline{y}_{.j}+\\overline{y}_{..}$$\n",
        "\n",
        "\n",
        "In the next section, we will show how the residuals are used in **model adequacy checking**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atyd_WxvUVk9",
        "colab_type": "text"
      },
      "source": [
        "**Multiple  Comparisons**. If the treatments in an RCBD are fixed, and the analysis indicates a significant difference in treatment means, the experimenter is usually interested in multiple comparisons to discover which treatment means differ. Any of the multiple comparison procedures discussed in  Section 3.4 may be used for this purpose. In the formulas of Section 3.4, simply replace the number of replicates in the single-factor completely randomized design ($n$) by the number of blocks ($b$). Also, remember to use the number of error degrees of freedom for the randomized block [$(a-\u00021)(b-\u00021)$] instead of those for the completely randomized design [$(N-a)$].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0b4y7eKottZ",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.2 Checking assumptions of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmmq_TtGotvs",
        "colab_type": "text"
      },
      "source": [
        "Generally, we should be alert for potential problems with the normality assumption, unequal error variance by treatment or block, and block–treatment interaction. As in the completely randomized design, residual analysis is the major tool used in this diagnostic checking. \n",
        "\n",
        "A normal probability plot of these residuals is shown in Figure 4.4. There is no indication of nonnormality, nor is there any evidence pointing to possible outliers. Figure 4.5 plots the residuals versus the fitted values $\\hat{y}_{ij}$. There should be no relationship between the size of the residuals and the fitted values $\\hat{y}_{ij}$. This plot reveals nothing of unusual interest.\n",
        "\n",
        "![alt text](https://)\n",
        "\n",
        "Figure 4.6 shows plots of the residuals by treatment and by block. These plots are potentially very informative. If there is more scatter in the residuals for a particular treatment, that could indicate that this treatment produces more erratic response readings than the others. More scatter in the residuals for a particular block could indicate that the block is not homogeneous. However, in our example, Figure 4.6 gives no indication of inequality of variance by treatment but there is an indication that there is less variability in the yield for batch 6. However, since all of the other residual plots are satisfactory, we will ignore this.\n",
        "\n",
        "![alt text](https://)\n",
        "\n",
        "\n",
        "Sometimes the plot of residuals versus $\\hat{y}_{ij}$ has a curvilinear shape;  for example, there may be a tendency for negative residuals to occur with low values, positive residuals with intermediate values, and negative residuals with high values. This type of pattern is suggestive of **interaction** between blocks and treatments. If this pattern occurs, a transformation should be used to eliminate or minimize the interaction. In Section 5.3.7,  we describe a statistical test that can be used to detect the presence of interaction in a randomized block design."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aEDaVZNpLlt",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.3 Estimating model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rre9Fc-o97a",
        "colab_type": "text"
      },
      "source": [
        "If both treatments and blocks are fixed, we may estimate the parameters in the RCBD model by least squares. Recall that the linear statistical model is\n",
        "\n",
        "$$y_{ij} = \\mu+\\tau_i + \\beta_j +\\epsilon_{ij}; i=1,2,...,a; j=1,...,b$$\n",
        "\n",
        "We can find the normal equations for an experimental design model. There are two linear dependencies in the normal equations, implying that two constraints must be imposed to solve these equations. The usual constraints are\n",
        "\n",
        "\n",
        "$$\\sum_{i=1}^{a}\\hat{\\tau}_i=0 \\text{ and } \\sum_{i=1}^{b}\\hat{\\beta}_j=0$$\n",
        "\n",
        "Using these constraints helps simplify the normal equations considerably and we obtain that\n",
        "\n",
        "$$\\hat{\\mu}=\\overline{y}_{..}$$ $$\\hat{\\tau_i}=\\overline{y}_{i.}-\\overline{y}_{..}$$ $$\\hat{\\beta_j}=\\overline{y}_{.j}-\\overline{y}_{..}$$\n",
        "\n",
        "\n",
        "Using the solution to the normal equation, we may find the estimated or fitted values of $y_{ij}$ as\n",
        "\n",
        "$$\\hat{y}_{ij}=\\hat{\\mu}+\\hat{\\tau_i}+\\hat{\\beta_j}=\\overline{y}_{i.}+\\overline{y}_{.j}-\\overline{y}_{..}$$\n",
        "\n",
        "This result was used previously in for computing the residuals from a randomized block design."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL98WDuqkKJq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxPKvQo_kKMW",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 The latin square design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E83O1d-4xujf",
        "colab_type": "text"
      },
      "source": [
        "The Latin square design is used to eliminate two nuisance sources of variability; that is, it systematically allows blocking in two directions.  Thus, the rows and columns represent two **restrictions on randomization**. In general, a Latin square for $p$ factors, or a $p$ x $\u0007p$ Latin square, is a square containing $p$ rows and $p$ columns. Each of the resulting $p^2$ cells contains one of the $p$ letters that correspond to the treatments, and each letter occurs once and only once in each row and column. Some examples of Latin squares are\n",
        "![alt text](https://methods.sagepub.com/images/virtual/experimental-design/9781412974455-p672-1.jpg)\n",
        "\n",
        "The **statistical model** for a Latin square is\n",
        "\n",
        "$$y_{ijk} = \\mu+\\alpha_i+\\tau_j + \\beta_k +\\epsilon_{ijk} \\text{ with } i,j,k=1,2,...,p$$ \n",
        "\n",
        "where $y_{ijk}$ is the observation in the $i$th row and $k$th column for the $j$th treatment, $\\mu$ \u0005is the overall mean, $\\alpha_i$ is the $i$th row effect,\u000f$\\tau_j$ is the $j$th treatment effect,\u0003$\\beta_k$ is the $k$th column effect, and\b $\\epsilon_{ijk}$ is the random error. Note that this is an **effects model**. The model is completely **additive**; that is, there is no interaction between rows, columns, and treatments. Because there is only one observation in each cell, only two of the three subscripts $i$,$j$, and $k$ are needed to denote a particular observation.\n",
        "\n",
        "Also, we suppose\n",
        "\n",
        "$$\\sum_{i=1}^{p}\\alpha_i= \\sum_{j=1}^{p}\\tau_j= \\sum_{k=1}^{p}\\beta_k=0$$\n",
        "\n",
        "For example, if $p=3$ the observations would be\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://i.ibb.co/h7y4NR1/ex-l-s.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "496dJjm9xunz",
        "colab_type": "text"
      },
      "source": [
        "The analysis of variance consists of partitioning the total sum of squares of the $N=\u0005p^2$ observations into components for rows, columns, treatments, and error, for example,\n",
        "$$\n",
        "SS_T = SS_{Rows} + SS_{Columns} + SS_{Treatments} + SS_E\n",
        "$$\n",
        "\n",
        "with respective degrees of freedom\n",
        "\n",
        "$$p^2-1 = p-1 + p-1 + p-1 + (p-2)(p-1)$$\n",
        "\n",
        "Under the usual assumption that \b$\\epsilon_{ijk}$ is NID (0,\u0002$\\sigma^2$), each sum of squares on the right-hand side of the equation  is, upon division by \u0002$\\sigma^2$, an independently distributed chi-square random variable. For testing no differences in treatment means \n",
        "\n",
        "$$\n",
        "H_0: \\beta_1=\\beta_2=...=\\beta_p=0 \\text{ (the factor } \\beta \\text{ does not influence)} \\\\\n",
        "H_1: \\beta_i\\neq 0 \\text{ for at least one }i \\text{ (the factor } \\beta \\text{ influence)}\n",
        "$$\n",
        "\n",
        "the appropriate statistic is\n",
        "\n",
        "$$F_0=\\frac{MS_{Treatments}}{MS_E}$$\n",
        "\n",
        "which is distributed as $F_{p-\u00021,(p-\u00022)(p-\u00021)}$ under the null hypothesis. So we reject the null hypothesis $H_0$ if $F_0>F_{p-\u00021,(p-\u00022)(p-\u00021);\\alpha}$.\n",
        "\n",
        "We may also test for no row effect and no column effect by forming the ratio of $MS_{Rows}$ or $MS_{Columns}$ to $MS_E$. However, because the rows and columns represent restrictions on randomization, these tests may not be appropriate. The computational procedure for the ANOVA in terms of treatment, row, and column totals is shown in Table 4.10. \n",
        "\n",
        "![alt text](https://i.ibb.co/f1mxxj1/anova-lsd.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSja0QURxurn",
        "colab_type": "text"
      },
      "source": [
        "As in any design problem, the experimenter should investigate the adequacy of the model by inspecting and plotting the residuals. For a Latin square, the parameters estimated are \n",
        "\n",
        "$$\\hat{\\mu}=\\overline{y}_{...}=\\frac{1}{p^2}\\sum_{i=1}^{p}\\sum_{j=1}^{p}\\sum_{k=1}^{p}y_{ijk}$$\n",
        "\n",
        "$$\\hat{\\alpha_i}=\\overline{y}_{i..}-\\overline{y}_{...}=\\frac{1}{p}\\sum_{j=1}^{p}\\sum_{k=1}^{p}y_{ijk}-\\overline{y}_{...}$$\n",
        "\n",
        "$$\\hat{\\tau_j}=\\overline{y}_{.j.}-\\overline{y}_{...}=\\frac{1}{p}\\sum_{i=1}^{p}\\sum_{k=1}^{p}y_{ijk}-\\overline{y}_{...}$$ \n",
        "\n",
        "\n",
        "$$\\hat{\\beta_k}=\\overline{y}_{..k}-\\overline{y}_{...}=\\frac{1}{p}\\sum_{i=1}^{p}\\sum_{j=1}^{p}y_{ijk}-\\overline{y}_{...}$$\n",
        "\n",
        "so the residuals are given by\n",
        "\n",
        "$$e_{ijk}=y_{ijk}-\\hat{y}_{ijk}=y_{ijk}-\\hat{\\mu}-\\hat{\\alpha_i}-\\hat{\\tau_j}-\\hat{\\beta_k}=y_{ijk}-\\overline{y}_{i..}-\\overline{y}_{.j.}-\\overline{y}_{..k}+2\\overline{y}_{...}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxAgtuMCoXLH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6NHEKz1oXV9",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 The graeco-latin square design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdoAyAtuoXX3",
        "colab_type": "text"
      },
      "source": [
        "Consider a $p$ x $\u0007p$ Latin square, and superimpose on it a second $p$ x $\u0007p$ Latin square in which the treatments are denoted by Greek letters. If the two squares when superimposed have the property that each Greek letter appears once and only once with each Latin letter, the two Latin squares are said to be **orthogonal**, and the design obtained is called a **Graeco-Latin square**. An example of a $4x\u00074$ Graeco-Latin square is shown in Table 4.18.\n",
        "\n",
        "![alt text](https://i.ibb.co/m8D7kNH/gls.png)\n",
        "\n",
        "The Graeco-Latin square design can be used to control systematically three sources of extraneous variability, that is, to block in *three* directions. The design allows investigation of four factors (rows, columns, Latin letters, and Greek letters), each at $p$ levels in only $p^2$ runs.Graeco-Latin squares exist for all $p\\geq3$ except $p=\u00056$ because only exist a Latin square. The statistical model for the Graeco-Latin square design is\n",
        "\n",
        "$$y_{ijkl} = \\mu+\\theta_i+\\tau_j + \\omega_k+ \\psi_l +\\epsilon_{ijkl} \\text{ with } i,j,k,l=1,2,...,p$$ \n",
        "\n",
        "\n",
        "where $y_{ijkl}$ is the observation in row $i$ and column $l$ for Latin letter $j$ and Greek letter $k$, $\\theta_i$ is the effect of the $i$th row,\u000f$\\tau_j$ is the effect of Latin letter treatment $j$,\u0015 $\\omega_k$ is the effect of Greek letter treatment $k$,\u0019 $\\psi_l$ is the effect of column $l$, and \b$\\epsilon_{ijkl}$ is an NID (0,$\\sigma^\u00022$) random error component. Only two of the four subscripts are necessary to completely identify an observation.\n",
        "\n",
        "The analysis of variance is very similar to that of a Latin square. Because the Greek letters appear exactly once in each row and column and exactly once with each Latin letter, the factor represented by the Greek letters is orthogonal to rows, columns, and Latin letter treatments. Therefore, a sum of squares due to the Greek letter factor may be computed from the Greek letter totals, and the experimental error is further reduced by this amount. The computational details are illustrated in Table 4.19.\n",
        "\n",
        "\n",
        "\n",
        "The null hypothesis of equal row, column, Latin letter, and Greek letter treatments would be tested by dividing the corresponding mean square by mean square error. The rejection region is the upper tail point of the $F_{p-\u00021,(p-\u00023)(p-\u00021)}$ distribution.\n",
        "\n",
        "\n",
        "![alt text](https://i.ibb.co/8xt10c4/glsd.png)\n",
        "\n"
      ]
    }
  ]
}