{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_design_of_experiments_comparative_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMSMrZh4p+fxgTaiXxOFz0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/design_of_experiments/blob/master/2_design_of_experiments_comparative_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVILFKw0e3xJ",
        "colab_type": "text"
      },
      "source": [
        "# 2. Comparative experiments\n",
        "\n",
        "In this chapter, we consider experiments to compare two conditions (sometimes called **treatments**). These are often called comparative experiments. We begin with an example of an experiment performed to determine whether two different formulations of a product give equivalent results. Later we do a review of several basic statistical concepts, such as random variables, probability distributions, random samples, sampling distributions, and tests of hypotheses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoy1qcjcz6E",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFTzxzSlc4Co",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "A company are interested to know the influence that the main image of their website has about the click-through rate of visitors on their website. The click-through rate is the number of visitors who click-through into the site divided by the total number of visitors to the site. Proper selection of the important factors can lead to an optimal web page design. In a real case, there are a lot of factors that can influence the response variable but for simplicity reasons, we suppose that the company only are interested in the influence of the main page and others do not matter. There are two choices for the main page.\n",
        "\n",
        "The experimenter prepared 10 samples of the first image and 10 samples of the second image. We will refer to the two different images as two **treatments** or as two **levels** of the **factor** main image.\n",
        "\n",
        "The click-through rate data from this experiment are shown in Table 2.1. \n",
        "\n",
        "\n",
        "![](https://i.ibb.co/XSWKn2y/img-ex-1.png)\n",
        "\n",
        "\n",
        "The graph in Figure 2.1 is called a **dot diagram**.\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/c1GHdFT/img-ex-2.png)\n",
        "\n",
        "\n",
        "Visual examination of these data gives\n",
        "the impression that the rate of the second image may be greater than the rate of the first image. This impression is supported by comparing the average rates, $\\overline{y}_1$= 16.76 for the first image and $\\overline{y}_2=$ 17.04 for the second image. The average rates in these two samples differ by what seems to be a modest amount. However, it is not obvious that this difference is large enough to imply that the two samples really are different. Perhaps this observed difference in average is the result of sampling fluctuation and the two samples are really\n",
        "identical. Possibly another two samples would give opposite results.\n",
        "A technique of statistical inference called hypothesis testing can be used to assist the experimenter in comparing these two samples. Hypothesis testing allows the comparison of the two samples to be made on objective terms, with knowledge of the\n",
        "risks associated with reaching the wrong conclusion. Before presenting procedures for hypothesis testing in simple comparative experiments, we will briefly summarize some basic statistical concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyn33qque35h",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Basic statistical concepts\n",
        "\n",
        "Each of the observations in an experiment described would be called a run. Notice that the individual runs differ, so there is fluctuation, or noise, in the observed response. This noise is usually called experimental error or simply error. It is a statistical error, meaning that it arises from variation that is uncontrolled and generally\n",
        "unavoidable. The presence of error or noise implies that the response variable is a random variable. A random variable may be either discrete or continuous. If the set of all possible values of the random variable is either finite or countably infinite, then the random variable is **discrete**, whereas if the set of all possible values of the random variable is an interval, then the random variable is **continuous**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hxv1eyXO4Z1",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1 Graphical description of variability\n",
        "\n",
        "- The **dot diagram**, illustrated in Figure 2.1 in the previous example, is a very useful device for displaying a small body of data (say up to about 20 observations). The dot diagram enables the experimenter to see quickly the general location or central tendency\n",
        "of the observations and their spread or variability.\n",
        "\n",
        "- If the data are fairly numerous, the dots in a dot diagram become difficult to distinguish and a [histogram](https://en.wikipedia.org/wiki/Histogram). The histogram shows the central tendency, spread, and general shape of the distribution of the data. Recall that a histogram is constructed by dividing the horizontal axis into bins (usually of equal length) and drawing a rectangle over the $j$th bin with the area of the rectangle proportional to $n_{j}$ , the number of observations that fall in that bin. A histogram is a large-sample tool. When the sample size is small the shape of the histogram can be very sensitive to the number of bins, the width of the bins, and the starting value for the first bin. Histograms should not be used with fewer than 75–100 observations.\n",
        "\n",
        "- The **box plot** (or box-and-whisker plot) is a very useful way to display data. A box plot displays the minimum, the maximum, the lower and upper quartiles (the 25th percentile and the 75th percentile, respectively), and the median (the 50th percentile) on a rectangular box aligned either horizontally or vertically.\n",
        "\n",
        "![](https://i.ibb.co/mN7ZWFS/img-ex-3.png)\n",
        "\n",
        "Dot diagrams, histograms, and box plots are useful for summarizing the information in\n",
        "a sample of data. To describe the observations that might occur in a sample more completely,\n",
        "we use the concept of the probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB9dpxF8QJxQ",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2 Probability distributions\n",
        "\n",
        "The probability structure of a random variable, say $y$, is described by its **probability distribution**. If $y$ is discrete, we often call the probability distribution of $y$, say $p(y)$, the probability mass function of $y$. If $y$ is continuous, the probability distribution of $y$, say $f(y)$, is often called the probability density function for $y$.\n",
        "\n",
        "A probability distribution is specified by two terms: the sample space, which is the set of all possible values of the random variable; and the probability that the random variable takes each of these values.\n",
        "\n",
        "Figure  2.4  illustrates the hypothetical discrete and continuous probability distributions. Notice that in the discrete probability distribution, the function $p(y_j)$ represents the probability, whereas, in the continuous, it is the area under the curve $f(y)$ associated with a given interval that represents probability(remember that for a continuous variable the probability that the random variable take one value is 0 since sample space is infinite).\n",
        "\n",
        "![alt text](https://i.ibb.co/xsKW87Y/distributions.png)\n",
        "\n",
        "The probability distributions have certain properties can be seen in the [resource](http://faculty.business.utsa.edu/manderso/STA4723/readings/Douglas-C.-Montgomery-Design-and-Analysis-of-Experiments-Wiley-2012.pdf). Also you can see the most popular known distributions which we will use as the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), the [chi-square distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution), the [Student's_t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) or the [F-distribution](https://en.wikipedia.org/wiki/F-distribution)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUY36qaTe38w",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3 Mean, variance and expected values\n",
        "\n",
        "- The mean, $\\mu$, of a probability distribution\n",
        "is a measure of its central tendency or location.\n",
        "\n",
        "- We may also express the mean in terms of the expected value or the long-run average value\n",
        "of the random variable $y$ as $\\mu=E(y)$, where $E$ denotes the expected value operator.\n",
        "\n",
        "- The variability or dispersion of a probability distribution can be measured by the variance, defined as $\\sigma^2$\n",
        "\n",
        "\n",
        "The concepts of expected value and variance are used extensively throughout this article, and it may be helpful to review several elementary results concerning these operators. These results can be seen in the [resource](http://faculty.business.utsa.edu/manderso/STA4723/readings/Douglas-C.-Montgomery-Design-and-Analysis-of-Experiments-Wiley-2012.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1LoPtbe4Cr",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Sampling and Sampling Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIAJILABkkvi",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1 Random samples, sample mean and sample variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gM52-QMRfEY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The objective of statistical inference is to make conclusions about a population using a sample from that population.\n",
        "Most of the methods that we will study assume that random samples are used. A random sample is a sample that has been selected from the population in such a way that every possible sample has an equal probability of being selected.\n",
        "\n",
        "Statistical inference makes use of quantities computed from the observations in the sample. We define a statistic as any function of the observations in a sample that does not contain unknown parameters. For example, suppose that $y_1, y_2 , . . . , y_n$ represents a sample. Then the **sample mean** $\\overline{y}$ and the **sample variance** $S^2$ are both statistics.\n",
        "\n",
        "$\\overline{y} = \\frac{\\sum_{i=1}y_i}{n}$ , $S^2 =\\frac{\\sum_{i=1}(y_i-\\overline{y})^2}{n-1}$\n",
        "\n",
        "These quantities are measures of the central tendency and dispersion of the\n",
        "sample, respectively. Sometimes $S=\\sqrt{S^2}$ , called the **sample standard deviation**, is used as a measure of dispersion. Experimenters often prefer to use the standard deviation to measure\n",
        "dispersion because its units are the same as those for the variable of interest $y$.\n",
        "\n",
        "The sample mean $\\overline{y}$  is a point estimator of the population mean $\\mu$, and the sample variance $S^2$ is a point estimator of the population variance $\\sigma^2$ . In general, an **estimator** of an unknown parameter is a statistic. Note that a point estimator is a random variable. A particular numerical value of an estimator, computed from sample data, is called an **estimate**. Several properties are required of good point estimators. Two of the most important are the following:\n",
        "- The point estimator should be **unbiased**. That is, the long-run average or expected value of the point estimator should be equal to the parameter that is being estimated.\n",
        "- An unbiased estimator should have **minimum variance**. This property states that the minimum variance point estimator has a variance that is smaller than the variance of any other estimator of that parameter.\n",
        "\n",
        "The statistics $\\overline{y}$ and $S^2$ are unbiased estimators of $\\mu$ and $\\sigma^2$, respectively, since $E(\\overline{y})=\\mu$ and $E(S^2)=\\sigma^2$. The proof of these results can be seen in the [resource](http://faculty.business.utsa.edu/manderso/STA4723/readings/Douglas-C.-Montgomery-Design-and-Analysis-of-Experiments-Wiley-2012.pdf).\n",
        "\n",
        "\n",
        "**Degrees of freedom**: We can see that $E(S^2)=\\frac{n}{n-1}E(SS)=\\sigma^2$\n",
        "where $SS=\\sum{}{}\\sum{}{}(y_{i}-\\overline{y})^2$ is  the  **corrected  sum  of  squares**  of  the  sample  observations. The quantity $n-\u00021$ in this equation is called the **number of degrees of freedom** of the sum of squares $SS$. This is a very general result; that is, if $y$ is a random variable with variance \u0002$\\sigma^2$ and $SS\u0005$ has $v$ degrees of freedom, then $E(\\frac{SS}{v})=\\sigma^2$\n",
        "\n",
        "The number of degrees of freedom of a sum of squares is equal to the number of independent elements in that sum of squares. For example, $SS$ \u0005consists of the sum of squares of the $n$ elements $y_1-\\overline{y}\u0002,y_2-\\overline{y}\u0002,...,y_n-\\overline{y}$\u0002. These elements are not all  independent  because  $SS=\\sum{}{}\\sum{}{}(y_{i}-\\overline{y})=0$; in  fact, only $n-\u00021$  of  them are independent, implying that $SS$ has $n-\u00021$ degrees of freedom.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-HSAFDrRfHP",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Inferences About the Differences in Means, Randomized Designs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2OGhpjbFqr",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to return to the click-through rate problem posed in Section 2.1. Recall that two different samples were being investigated to determine if they differ. In this section, we discuss how the data from this simple comparative experiment can be analyzed using **hypothesis testing** and **confidence interval** procedures for comparing two treatment means.\n",
        "\n",
        "\n",
        "Throughout this section, we assume that a completely randomized experimental design is used. In such a design, the data are usually viewed as if they were a random sample from a normal distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yzbXl-vbQsI",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1 Hypothesis Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqpJZSTbbrQR",
        "colab_type": "text"
      },
      "source": [
        "In general, we can think of these two formulations as two levels of the factor “formulations.” Let $y_{11} , y_{12} , . . . , y_{1n_{1}}$ represent the $n_{1}$ observations from the first factor level and $y_{21} , y_{22} , . . . , y_{2n_{2}}$ represent the $n_{2}$ observations from the second factor level. We assume that the samples are drawn at random from two independent normal populations.\n",
        "Figure 2.9 illustrates the situation.\n",
        "\n",
        "\n",
        "A **Model for the Data**. We often describe the results of an experiment with a model. A simple statistical model that describes the data from an experiment such as we have just described is:\n",
        "\n",
        "$y_{ij} = \\mu_{i} + \\epsilon_{ij}; i=1,2; j=1,...,n_{i}$\n",
        "where $y_{ij}$ is the $j$th observation from factor level $i$, $\\mu_{i}$ is the mean of the response at the $i$th factor level, and $\\epsilon_{ij}$ is a normal random variable associated with the $i$jth observation. We assume that $\\epsilon_{ij}$ are NID(0, $\\sigma_{i}^{2}$ ), $i=1, 2$. It is customary to refer to $\\epsilon_{ij}$ as the random error component of the model. Because the means $\\mu_{i}$ are constants, we see directly from the model that $y_{ij}$ are NID($\\mu_{i}$ , $\\sigma_{i}^{2}$ ), $i= 1, 2$, just as we previously assumed. \n",
        "\n",
        "![alt text](https://i.ibb.co/fYcJ39h/aaaaaaaa.png)\n",
        "\n",
        "A **statistical hypothesis** is a statement either about the parameters of a probability distribution or the parameters of a model. The hypothesis\n",
        "reflects some conjecture about the problem situation. For example, we may think that the mean of the two formulations are equal. This may be stated formally as\n",
        "$$\n",
        "H_0: \\mu_1=\\mu_2\\\\\n",
        "H_1: \\mu_1\\neq\\mu_2\n",
        "$$\n",
        "where $\\mu_1$ is the mean of the first sample and $\\mu_2$ is the mean of the second sample. The statement $H_0: \\mu_1=\\mu_2$ is called the null hypothesis and $H_1: \\mu_1\\neq\\mu_2$ is called the alternative hypothesis. The alternative hypothesis specified here is called a two-sided alternative hypothesis because it would be true if $\\mu_1<\\mu_2$ or if $\\mu_1>\\mu_2$.\n",
        "\n",
        "Two kinds of errors may be committed when testing hypotheses. If the null hypothesis is rejected when it is true, a type I error has occurred. If the null hypothesis is not rejected when it is false, a type II error has been made. The probabilities of these two errors are given\n",
        "special symbols\n",
        "\n",
        "$$\n",
        "\\alpha=P(type I error)\\\\\n",
        "\\beta=P(type II error)\n",
        "$$\n",
        "\n",
        "The general procedure in hypothesis testing is to specify a value of the probability of type I error $\\alpha$, often called the significance level of the test, and then design the test procedure so that the probability of type II error $\\beta$ has a suitably small value.\n",
        "\n",
        "**The Two-Sample t-Test**. Suppose that we could assume that the variances were identical for both treatments. Then the appropriate test statistic to use for comparing two treatment means in the completely randomized design is\n",
        "\n",
        "$t_o=\\frac{\\overline{y_1}-\\overline{y_2}}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}$\n",
        "\n",
        "where $\\overline{y_1}$ and $\\overline{y_2}$ are the sample means, $n_1$ and $n_2$ are the sample sizes, $S^2_p$ is an estimate of the common variance $\\sigma^2_1=\\sigma^2_2=\\sigma^2$.\n",
        "\n",
        "To determine whether to reject $H_0$, we would compare $t_0$ to the $t$ distribution with $n_1+n_2+2$ degrees of freedom. If $|t_0|> t_{\\frac{\\alpha}{2},n_1+n_2-2}$, where $t_{\\frac{\\alpha}{2},n_1+n_2-2}$ is the upper $\\frac{\\alpha}{2}$ percentage point of the $t$ distribution with $n_1+n_2-2$ degrees of freedom, we would reject $H_0$ and conclude that the mean of the two samples differ. This test procedure is usually called the **two-sample t-test**.\n",
        "\n",
        "To illustrate the procedure, consider the click-through rate data in Table 2.1. For these data, we find that\n",
        "\n",
        "\n",
        "\n",
        "$t_o=\\frac{\\overline{y_1}-\\overline{y_2}}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}=-2.2$\n",
        "\n",
        "Because $|t_0|=\u00022.20>t_{0.025,18}=2.101$, we would reject $H_0$ and conclude that the mean of the treatments are different.\n",
        "\n",
        "![alt text](https://i.ibb.co/0hfJqrC/ccccdc.png)\n",
        "\n",
        "\n",
        "\n",
        "The Use of **P-Values** in Hypothesis Testing.\n",
        "The P-value is the probability that the test statistic will take on a value that is at least as\n",
        "extreme as the observed value of the statistic when the null hypothesis $H_0$ is true. Thus, a P-\n",
        "value conveys much information about the weight of evidence against $H_0$ , and so a decision-maker can draw a conclusion at any specified level of significance. More formally, we define the P-value as the smallest level of significance that would lead to rejection of the null hypothesis $H_0$. It is customary to call the test statistic (and the data) significant when the null hypothesis $H_0$ is rejected; therefore, we may think of the P-value as the smallest level at which the data are significant. Once the P-value is known, the decision-maker can determine how significant the data are without the data analyst formally imposing a preselected level of significance. It is not always easy to compute the exact P-value for a test. However, most modern computer programs for statistical analysis report P-values.\n",
        "\n",
        "For the previous example, the p-value has been calculated using a software program resulting in $P=\u00050.0411$. Thus the null hypothesis $H_0: \\mu_1=\\mu_2$ would be rejected at any level of significance $\\alpha>0.0411$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMA7rViDr0Ij",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2 Confidence Intervals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj4uF1cMNmdn",
        "colab_type": "text"
      },
      "source": [
        "Although hypothesis testing is a useful procedure, it sometimes does not tell the entire story. It is often preferable to provide an interval within which the value of the parameter or parameters in question would be expected to lie. These interval statements are called **confidence intervals**. In many experiments, the experimenter already knows that the means \u0005$\\mu_1$ and $\\mu_\u00052$ differ; consequently, hypothesis testing on \u0005$\\mu_1=\\mu_2$ is of little interest. The experimenter would usually be more interested in knowing how much the means differ. A confidence interval on the difference in means \u0005$\\mu_1-\\mu_2$ is used in answering this question.\n",
        "\n",
        "To define a confidence interval, suppose that $\\theta$ is an unknown parameter. To obtain an interval estimate of $\\theta$, we need to find two statistics $L$ and $U$ such that the probability statement $$P(L\\leq \\theta\\leq U)=1-\\alpha$$ is true. The interval $$L\\leq \\theta\\leq U$$ is called a $100(1-\\alpha\u0004$\u0002) **percent confidence interval** for the parameter $\\theta$. The interpretation of this interval is that if, in repeated random samplings, a large number of such intervals are constructed, $100(1-\\alpha\u0002)$ percent of them will contain the true value of $\\theta$ . The statistics $L$ and $U$ are called the lower and upper confidence limits, respectively, and $1-\\alpha$ is called the confidence coefficient. \n",
        "\n",
        "Suppose that we wish to find a $100(1-\\alpha\u0004$\u0002) percent confidence interval on the true difference in means \u0005$\\mu_1-\\mu_2$ for the previous problem. The interval can be derived in the following way. The statistic \n",
        "$$\\frac{\\overline{y_1}-\\overline{y_2}-(\\mu_1-\\mu_2)}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\sim t_{n_1+n_2-2}$$\n",
        "\n",
        "Thos we could prove that the $100(1-\\alpha\u0004$\u0002) percent confidence interval for $\\mu_1-\\mu_2$ is\n",
        "\n",
        "$$\\overline{y_1}-\\overline{y_2}-t_{n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\leq \\mu_1-\\mu_2 \\leq \\overline{y_1}-\\overline{y_2}+t_{n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}$$\n",
        "\n",
        "The  actual  95  percent  confidence  interval  estimate  for  the  difference  in  mean for  the  two treatments of the previous example is  found  by  substituting  in the equation:\n",
        "$$-0.55\\leq \\mu_1-\\mu_2 \\leq -0.01$$\n",
        "\n",
        "Note that because \u0005$\\mu_1-\\mu_2=0$ is not included in this interval, the data do not support the hypothesis that \u0005$\\mu_1=\\mu_2$ at the 5 percent level of significance (recall that the P-value for the two-sample t-test was 0.042, just slightly less than 0.05). The mean click-through rate of the second image likely exceeds the mean click-through rate of the first image. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qFCLwhMtG3M",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.3 The case of unequal variances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfoIzkskmHQg",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.5 Comparing a single mean to a specified value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkeDDsIymTLy",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.6 Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmc_-p2cmXtf",
        "colab_type": "text"
      },
      "source": [
        "Tables 2.4 and 2.5 summarize the t-test and z-test procedures discussed above  for samplemeans. Critical regions are shown for both two-sided and one-sided alternative hypotheses.\n",
        "\n",
        "![alt text](https://i.ibb.co/HXwd8dN/means1.png)\n",
        "\n",
        "![alt text](https://i.ibb.co/yfkL2zY/means2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuahvH9TRfKh",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Inferences about the differences in means( paired comparison designs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6H8xT-qRfNb",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Inferences about the variances of normal distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1FuW0-95S_K",
        "colab_type": "text"
      },
      "source": [
        "In many experiments, we are interested in possible differences in the mean response for two treatments. However, in some experiments, it is the comparison of variability in the data that is important. \n",
        "\n",
        "We now briefly examine tests of hypotheses and confidence intervals for variances of normal distributions. Unlike the tests on means, the procedures for tests on variances are rather sensitive to the normality assumption. \n",
        "\n",
        "Suppose we wish to test the hypothesis that the variance of a normal population equals a constant, for example $\\sigma_0^2$. Stated formally, we wish to test\n",
        "$$H_o:\\sigma^2=\\sigma_0^2$$\n",
        "$$H_o:\\sigma^2\\neq \\sigma_0^2$$\n",
        "\n",
        "The test statistic is \n",
        "$$\\chi_o^2=\\frac{SS}{\\sigma_0^2}=\\frac{(n-1)S^2}{\\sigma_0^2}$$\n",
        "\n",
        "where $SS=\\sum{}{}\\sum{}{}(y_{i}-\\overline{y})^2$ is  the  corrected  sum  of  squares  of  the  sample  observations.  The appropriate reference distribution for $\\chi_o^2$ is the chi-square distribution with $n-1$ degrees of freedom. The null hypothesis is rejected if $\\chi_o^2>\\chi_{\\frac{\\alpha}{2},n-1}^2$\n",
        "\n",
        "We might get the $100(1-\\alpha)$ percent confidence interval on $\\sigma^2$\n",
        "\n",
        "Now consider testing the equality of the variances of two normal populations. If independent random samples of size $n1$ and $n2$ are taken from populations 1 and 2, respectively,the test statistic for\n",
        "$$H_o:\\sigma_1^2=\\sigma_2^2$$\n",
        "$$H_o:\\sigma_1^2\\neq \\sigma_2^2$$\n",
        "is the ratio of the sample variances $F_0=\\frac{S_1^2}{S_2^2}$\n",
        "\n",
        "The  appropriate  reference  distribution  for  $F_0$ is  the  $F$ distribution with $n_1-1$  numerator degrees of freedom and $n_2-1$ denominator degrees of freedom. The null hypothesis would be  rejected  if  \n",
        "$$F_0>F_{\\frac{\\alpha}{2},n_1-1,n_2-1}$$ \n",
        "\n",
        "Critical values for the one-sided alternative hypothesis are given in Table 2.8. Test procedures for more than two variances are discussed in Section 3.4.3.\n",
        "\n",
        "We can get the $100(1-\\alpha)$ confidence interval for the ratio of the population variances $\\frac{\\sigma_1^2}{\\sigma_2^2}$.\n",
        "\n",
        "![alt text](https://i.ibb.co/Z1jKJpZ/gggggg.png)"
      ]
    }
  ]
}