{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_design_of_experiments_single_factor.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u3kTCG4AdGcZ",
        "IheINqjVjkVO",
        "3WrLxB4TjdBb"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMgpFvU6R+baGP8dLHQu05B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/design_of_experiments/blob/master/3_design_of_experiments_single_factor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF0pK9mYRfVg",
        "colab_type": "text"
      },
      "source": [
        "# 3. Experiments with a single factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJm4btkHe4F0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This chapter focuses on methods for the design and analysis of single factor experiments with an arbitrary number of levels of the factor (or $a$ treatments). We will assume that the experiment has been completely randomized.\n",
        "\n",
        "- Example: A company are interested to know the influence that the main image of their website has about the click-through rate of visitors on their website. The click-through rate is the number of visitors who click-through into the site divided by the total number of visitors to the site. In a real case, there are a lot of factors that can influence the response variable but for simplicity reasons, we suppose that the company only are interested in the influence of the main page and others do not matter. There are two choices for the main page. The click-through rate data from this experiment are shown in Table 2.1. \n",
        "\n",
        "\n",
        "![](https://i.ibb.co/XSWKn2y/img-ex-1.png)\n",
        "\n",
        "\n",
        "\n",
        "The appropriate procedure for testing the equality of several means(two in the example) is the analysis of variance. However, the analysis of variance has a much wider application than the problem above. It is probably the most useful technique in the field of statistical inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J1CaBU8XDnW",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 The analysis of variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-2UfXj2XDsb",
        "colab_type": "text"
      },
      "source": [
        "Suppose we have $a$ **treatments** or different **levels** of a **single factor** that we wish to compare.\n",
        "The observed response from each of the $a$ treatments is a random variable. The data would appear\n",
        "as in Table 3.2. An entry in Table 3.2 (e.g., $y_{ij}$ ) represents the $j$th observation taken under factor level or treatment $i$. There will be, in general, n observations under the ith treatment. Notice that\n",
        "Table 3.2 is the general case of the data from the click-through rate experiment in Table 2.1.\n",
        "\n",
        "![alt text](https://i.ibb.co/HhKj3sD/i1.png)\n",
        "\n",
        "A **Model for the Data**. We often describe the results of an experiment with a model.\n",
        "A simple statistical model that describes the data from an experiment such as we have just described is:\n",
        "\n",
        "$$y_{ij} = \\mu_{i} + \\epsilon_{ij}; i=1,2,...,a; j=1,...,n$$    \n",
        "\n",
        "\n",
        "where $y_{ij}$ is the $j$th observation from factor level $i$, $\\mu_{i}$ is the mean of the response at the $i$th factor level, and $\\epsilon_{ij}$ is a normal random variable associated with the $i$jth observation. We assume that $\\epsilon_{ij}$ are NID(0, $\\sigma_{i}^{2}$ ), $i=1, 2$. It is customary to refer to $\\epsilon_{ij}$ as the random error component of the model. Because the means $\\mu_{i}$ are constants, we see directly from the model\n",
        "that $y_{ij}$ are NID($\\mu_{i}$ , $\\sigma_{i}^{2}$ ), $i= 1, 2$, just as we previously assumed. \n",
        "\n",
        "The previous equation is called the **means model**.\n",
        "An alternative way to write a model for the data is to define $\\mu_i = \\mu+\\tau_i$ so the previous equation becomes\n",
        "\n",
        "$$y_{ij} = \\mu+\\tau_i + \\epsilon_{ij}; i=1,2,...,a; j=1,...,n$$ \n",
        "\n",
        "In this form of the model, $\\mu$ is a parameter common to all treatments called the **overall mean**, and $\\tau_i$ is a parameter unique to the $i$th treatment called the $i$th **treatment effect**. The second equation is usually called the **effects model**.\n",
        "Both the means model and the effects model are linear statistical models; that is, the response variable $y_{ij}$ is a linear function of the model parameters. Although both forms of the model are useful, the effects model is more widely encountered in the experimental design literature. It has some intuitive appeal in that $\\mu$ is a constant and the treatment effects $\\tau_i$ represent deviations from this constant when the specific treatments are applied.\n",
        "This equation is also called the **one-way** or **single-factor analysis of variance (ANOVA)** model because only one factor is investigated.\n",
        "\n",
        "Furthermore, we will require that the experiment be performed in random order so that the environment in which the treatments\n",
        "are applied (the experimental units) is as uniform as possible. Thus, the experimental design is a **completely randomized design**. Our objectives will be to test appropriate hypotheses about the treatment means and to estimate them. For hypothesis testing, the\n",
        "model errors are assumed to be normally and independently distributed random variables with\n",
        "mean zero and variance $\\sigma^2$ . The variance $\\sigma^2$ is assumed to be constant for all levels of the fac-\n",
        "tor. This implies that the observations\n",
        "\n",
        "$$y_{ij}\\sim N(\\mu+\\tau_i , \\sigma^{2} )$$\n",
        "\n",
        "- **Fixed or Random Factor?** The statistical model explained describes two different situations concerning the treatment effects. First, the $a$ treatments could have been\n",
        "specifically chosen by the experimenter. In this situation, we wish to test hypotheses about the treatment means, and our conclusions will apply only to the factor levels considered in the\n",
        "analysis. The conclusions cannot be extended to similar treatments that were not explicitly considered. We may also wish to estimate the model parameters $(\\mu, \\tau_i , \\sigma^2 )$. This is called the **fixed effects model**. Alternatively, the $a$ treatments could be a random sample from a larger population of treatments. In this situation, we should like to be able to extend the conclusions (which are based on the sample of treatments) to all treatments in the population, whether or not they were explicitly considered in the analysis. Here, the $\\tau_i$ are random variables, and knowledge about the particular ones investigated is relatively useless. Instead, we test hypotheses about the variability of the $\\tau_i$ and try to estimate this variability. This is called the **random effects model** or components of the variance model. We discuss the single-factor random effects model in Section 3.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_HG_FpbXDyi",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Analysis of the Fixed Effects Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb50JaL1XD3K",
        "colab_type": "text"
      },
      "source": [
        "In this section, we develop the single-factor analysis of variance for the fixed effects model. Recall that $y_{i.}$ represents the total of the observations under the $i$th treatment. Let $\\overline{y_i.}$ represent the average of the observations under the $i$th treatment. Similarly, let $y_{..}$ represents the grand total of all the observations and $\\overline{y}_{..}$ represents the grand average of all the observations.\n",
        "Expressed symbolically,\n",
        "\n",
        "$$y_{i.}=\\sum_{j=1}^{n}y_{ij}$$  $$\\overline{y_i.}=\\frac{y_{i.}}{n}, i=1,...,a$$\n",
        "\n",
        "$$y_{..}=\\sum_{i=1}^{a}\\sum_{j=1}^{n}y_{ij}$$ $$\\overline{y_{..}}=\\frac{y_{..}}{N}$$\n",
        "\n",
        "where $N\u0005=an$ is the total number of observations.\n",
        "\n",
        "We are interested in testing the equality of the $a$ treatment means; that is, $E(y_{ij})=\\mu + \\tau_i=\\mu_i, \u000fi=1, 2, . . . ,a$. The appropriate hypotheses are\n",
        "\n",
        "$$\n",
        "H_0: \\mu_1=\\mu_2=...=\\mu_a\\\\\n",
        "H_1: \\mu_i\\neq\\mu_j \\text{ for at least one pair}(i,j)\n",
        "$$\n",
        "\n",
        "In the effects model, we break the $i$th treatment mean \u0005$\\mu_i$ into two components such that $\\mu_i=\\mu+\\tau_i$. We usually think of $\\mu$ as an overall mean so that\n",
        "\n",
        "$$\\mu=\\frac{\\sum_{i=1}^{a}\\mu_i}{a}$$\n",
        "\n",
        "This definition implies that\n",
        "\n",
        "$$\\sum_{i=1}^{a}\\tau_i=0$$\n",
        "\n",
        "That is, the treatment or factor effects can be thought of as deviations from the overall mean.\n",
        "Consequently, an equivalent way to write the above hypotheses is in terms of the treatment effects $\\tau_i$ \n",
        "\n",
        "$$\n",
        "H_0: \\tau_1=\\tau_2=...=\\tau_a=0\\\\\n",
        "H_1: \\tau_i\\neq0 \\text{ for  at  least  one }i\n",
        "$$\n",
        "\n",
        "Thus, we speak of testing the equality of treatment means or testing that the treatment effects (the $\\tau_i$ ) are zero. The appropriate procedure for testing the equality of a treatment means is the analysis of variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptW-xGiv6bMW",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.1 Decomposition of the total sum of squares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpoY-DSL6bVl",
        "colab_type": "text"
      },
      "source": [
        "The name **analysis of variance** is derived from a partitioning of total variability into its component parts. The total corrected sum of squares\n",
        "\n",
        "$$SS_T=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(y_{ij}-\\overline{y}_{..})^2$$\n",
        "\n",
        "is used as a measure of the overall variability in the data. Intuitively, this is reasonable because if we were to divide $SS_T$ by the appropriate number of degrees of freedom (in this case, $an-1=N-1$), we would have the **sample variance** of the $y$’s. The sample variance is, of course, a standard measure of variability.\n",
        "\n",
        "We can prove that\n",
        "\n",
        "$$SS_T=\\sum_{i=1}^{a}\\sum_{j=1}^{n}(y_{ij}-\\overline{y}_{..})^2= n\\sum_{i=1}^{a}(\\overline{y}_{i.}-\\overline{y}_{..})^2+\\sum_{i=1}^{a}\\sum_{j=1}^{n}(y_{ij}-\\overline{y}_{i.})^2$$\n",
        "\n",
        "This equation is the fundamental ANOVA identity. It states that the total variability in the data, as measured by the total corrected sum of squares, can be partitioned into a sum of squares of the differences between the treatment averages and the grand average plus a sum of squares of the differences of observations within treatments from the treatment average. Now, the difference between the observed treatment averages and the grand average is a measure of\n",
        "the differences between treatment means, whereas the differences of observations within $a$ treatment from the treatment average can be due to only random error. Thus, we may write equation symbolically as\n",
        "\n",
        "$$\n",
        "SS_T = SS_{Treatments} + SS_E\n",
        "$$\n",
        "\n",
        "where $SS_{Treatments}$ is called the sum of squares due to treatments (i.e., between treatments), and $SS_E$ is called the sum of squares due to error (i.e., within treatments). There are $an=N$ total observations; thus, $SS_T$ has $N-1$ degrees of freedom. There are $a$ levels of the factor (and $a$ treatment means), so $SS_{Treatments}$ has $a-1$ degrees of freedom. Finally, there are $n$ replicates within any treatment providing $n-1$ degrees of freedom with which to estimate the experimental error. Because there are $a$ treatments, we have $a(n-1)=an-a=N-a$ degrees of freedom for error.\n",
        "\n",
        "\n",
        "\n",
        "We can prove that\n",
        "\n",
        "$\\frac{SS_E}{(N-a)}$ is a pooled estimate of the common variance within each of the $a$ treatments.\n",
        "\n",
        "Similarly, if there were no differences between the $a$ treatment means, we could use the variation of the treatment averages from the grand average to estimate $\\sigma^2$ . Specifically, $\\frac{SS_{Treatments}}{(a-1)}$ is an estimate of \u0002$\\sigma^2$ if the treatment means are equal.\n",
        "\n",
        "We see that the ANOVA identity provides us with two estimates of $\\sigma^2$: one based on the inherent variability within treatments and the other based on the variability between treatments. If there are no differences in the treatment means, these\n",
        "two estimates should be very similar, and if they are not, we suspect that the observed difference must be caused by differences in the treatment means.\n",
        "\n",
        "We have used an intuitive argument to develop this result, but we can take a more formal approach. The quantities\n",
        "\n",
        "$$\n",
        "MS_{Treatments}=\\frac{SS_{Treatment}}{a-1}\n",
        "$$\n",
        "and\n",
        "$$\n",
        "MS_{E}=\\frac{SS_{E}}{N-a}\n",
        "$$\n",
        "are usually called **mean squares**. We can calculate the expected values of these quantities\n",
        "\n",
        "$$E(MS_{E})=\\sigma^2$$\n",
        "$$E(MS_{Treatments})=\\sigma^2+\\frac{n\\sum_{i=1}^{a}\\tau_i^2}{a-1}$$\n",
        "\n",
        "Thus, as we argued heuristically, $MS_{E}=\\frac{SS_{E}}{N-a}$ estimates \u0002$\\sigma^2$, and, if there are no differences in treatment means  (which  implies that $\\tau_i=0$), $MS_{Treatments}=\\frac{SS_{Treatment}}{a-1}$  also estimates\u0002 $\\sigma^2$. However, note that if treatment means do differ, the expected value of the treatment mean square is greater than \u0002$\\sigma^2$.It seems clear that a test of the hypothesis of no difference in treatment means can be performed by comparing $MS_{Treatments}$ and $MS_E$. We now consider how this comparison may be made.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3kTCG4AdGcZ",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.2 Statistical analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u6nuPtT6bYa",
        "colab_type": "text"
      },
      "source": [
        "We now investigate how a formal test of the hypothesis of no differences in treatment means\n",
        "($H_0:\\mu_1=\\mu_2=...=\\mu_a$ , or equivalently, $H_0:\\tau_1=\\tau_2=...=\\tau_a=0$) can be performed.\n",
        "Because we have assumed that the errors $\\epsilon_{ij}\\sim N(0,\\sigma^2)$ are normally and independently distributed, the observations $y_{ij}\\sim N(\\mu+\\tau_i,\\sigma^2)$ are normally and independently distributed. Thus, $SS_T$ is a sum of squares in normally distributed random variables; consequently, it can be shown that $\\frac{SS_T}{\\sigma^2}\\sim \\chi_{N-1}$, this is, is distributed as chi-square with $N-1$ degrees of freedom. Furthermore, we can show that $\\frac{SS_E}{\\sigma^2}\\sim \\chi_{N-a}$ and that $\\frac{SS_Treatments}{\\sigma^2}\\sim \\chi_{a-1}$ if the null hypothesis $H_0:\\tau_i=0$ is true. However, all three sums of squares are not necessarily independent because $SS_{Treatments}+SS_E=SS_T$.\n",
        "\n",
        "Using the [Cochran’s theorem](https://en.wikipedia.org/wiki/Cochran%27s_theorem) we can establish the independence of $SS_E$ and $SS_{Treatments}$. Specifically, \n",
        "Cochran’s theorem implies that $\\frac{SS_{Treatments}}{\\sigma^2}$ and $\\frac{SS_E}{\\sigma^2}$ are independently distributed chi-square random variables. Therefore, if the null hypothesis of no difference in treatment means is true, the ratio\n",
        "$$\n",
        "F_0=\\frac{\\frac{SS_{Treatments}}{a-1}}{\\frac{SS_E}{N-a}}=\\frac{MS_{Treatments}}{MS_E}\\sim F_{a-1 N-a}\n",
        "$$\n",
        "This equation is the **test statistic** for the hypothesis of no differences in treatment means.\n",
        "\n",
        "Also, under the null hypothesis, $E(MS_{Treatments})=\\sigma^2$ ($MS_{Treatments}$ is an unbiased estimator of $\\sigma^2$) . However, if the null hypothesis is false, $E(MS_{Treatments})>\\sigma^2$. Therefore, under the alternative hypothesis, \n",
        "$E(MS_{Treatments})>E(MS_{E})$\n",
        "and we should reject $H_0$ on values of the test statistic that are too large. Therefore, we should reject $H_0$ and conclude that there are differences in the treatment means if\n",
        "$$F_0>F_{\\alpha,a-1,N-a}$$\n",
        "\n",
        "Alternatively, we could use the P-value approach\n",
        "for decision making.\n",
        "\n",
        "![alt text](https://i.ibb.co/LnPKjY1/ffffff.png)\n",
        "\n",
        "In practice, we use computer software to do this. The test procedure is summarized in Table 3.3. This is called an **analysis of variance**(or **ANOVA**) table.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IheINqjVjkVO",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.3 Estimation of the model parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6BQRKBueE4h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We present estimators for the parameters in the single-factor model \n",
        "$$y_{ij} = \\mu+\\tau_i + \\epsilon_{ij}; i=1,2,...,a; j=1,...,n$$\n",
        "\n",
        "We can prove using the [ordinary least squares (OLS) method](https://en.wikipedia.org/wiki/Ordinary_least_squares) or the [maximum likelihood estimation (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) that reasonable estimates of the overall mean and the treatment effects are given by\n",
        "\n",
        "$$\\hat{\\mu}=\\overline{y}_{..}$$\n",
        "$$\\hat{\\tau_i}=\\overline{y}_{i.}-\\overline{y}_{..}$$\n",
        "\n",
        "Note that the overall mean is estimated by the grand average of the observations and that any treatment effect is just the difference\n",
        "between the treatment average and the grand average.\n",
        "\n",
        "\n",
        "A confidence interval estimate of the $i$th treatment mean could be easily determined. These confidence intervals expressions are **one-at-a-time** confidence intervals. That is, the confidence\n",
        "level $1-\\alpha$ applies to only one particular estimate. However, in many problems, the experimenter may wish to calculate simultaneous confidence intervals, one for each number of means or differences between means.\n",
        "\n",
        "The [Bonferroni method](https://en.wikipedia.org/wiki/Bonferroni_correction) allows the experimenter to construct a set of $r$ simultaneous confidence intervals on treatment means or differences in treatment means for which the overall confidence level is at least $100(1-\\alpha)$ percent. When $r$ is not too large, this is a very nice method that leads to reasonably short confidence intervals.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WrLxB4TjdBb",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.4 Unbalanced data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orHTdpY1jnay",
        "colab_type": "text"
      },
      "source": [
        "In some single-factor experiments, the number of observations taken within each treatment may be different. We then say that the design is **unbalanced**. The analysis of variance described above may still be used, but slight modifications must be made in the sum of\n",
        "squares formulas. Let $n_i$ observations be taken under treatment $i$ ($i=1,2,...,a$) and $N=\\sum_{i=1}^{a} n_i$\n",
        "The formulas for $SS_T$ and $SS_{Treatments}$ can be modified.\n",
        "\n",
        "There are two advantages in choosing a balanced design. First, the test statistic is relatively insensitive to small departures from the assumption of equal variances for the $a$ treatments if the sample sizes are equal. This is not the case for unequal sample sizes. Second, the power of the test is maximized if the samples are of equal size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffVbPe2BgbnU",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Checking assumptions of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flKDAnMB6ba_",
        "colab_type": "text"
      },
      "source": [
        "The decomposition of the variability in the observations through an analysis of variance is a purely algebraic relationship. However, the use of the test for no differences in treatment means requires that certain assumptions be satisfied.\n",
        "Specifically, these assumptions are that the observations are adequately described by the model\n",
        "\n",
        "$$y_{ij} = \\mu+\\tau_i + \\epsilon_{ij}$$\n",
        "\n",
        "and that the errors are normally and independently distributed  $\\epsilon_{ij}\\sim N(0,\\sigma^2)$. If these assumptions are valid, the analysis of variance procedure is an exact test of the hypothesis of no difference in treatment means.\n",
        "\n",
        "\n",
        "In practice, however, these assumptions will usually not hold exactly.\n",
        "Violations of the basic assumptions and model adequacy can be easily investigated by the examination of **residuals**. We define the residual for observation $j$ in treatment $i$ as\n",
        "$$e_{ij}=y_{ij}-\\hat{y}_{ij}$$\n",
        "\n",
        "where $\\hat{y}_{ij}$ is the estimate of the corresponding observation $y_{ij}$ obtained as follows:\n",
        "$$y_{ij} = \\hat{\\mu}+\\hat{\\tau}_i = \\overline{y}_{..}+(\\overline{y}_{i.}-\\overline{y}_{..})=\\overline{y}_{i.}$$\n",
        "\n",
        "This equation gives the intuitively appealing result that the estimate of any observation in the $i$th treatment is just the corresponding treatment average.\n",
        "\n",
        "Examination of the residuals should be an automatic part of any analysis of variance. If the model is adequate, the residuals should be structureless; that is, they should contain no obvious patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv5EWYEw6bod",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.1 The normality assumption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S63pWdxIbUJF",
        "colab_type": "text"
      },
      "source": [
        "A check of the normality assumption could be made by plotting a histogram of the residuals. If the assumption on the errors is satisfied, this plot should look like a sample from a normal distribution centered at zero. Unfortunately, with small samples, considerable fluctuation in the shape of a histogram often occurs, so the appearance of a moderate departure from normality does not necessarily imply a serious violation of the assumptions. Big deviations from normality are potentially serious and require further analysis.\n",
        "\n",
        "An extremely useful procedure is to construct a normal probability plot of the residuals. Recall from Chapter 2 that we used a normal probability plot to check the assumption of normality when using the t-test. In the analysis of variance, it is usually more effective (and straightforward) to do this with the residuals. If the error distribution is normal, this plot will resemble a straight line. In visualizing the straight line, place more emphasis on the central values of the plot than on the extremes.\n",
        "\n",
        "![alt text](https://i.ibb.co/Tc110MV/normal-plot.png)\n",
        "\n",
        "The general impression from examining this display is that the error distribution is approximately normal. The tendency of the normal probability plot to bend down slightly on the left side and upward slightly on the right side implies that the tails of the error distribution are somewhat *thinner* than would be anticipated in a normal distribution; that is, the largest residuals are not quite as large (in absolute value) as expected. This plot is not grossly nonnormal, however.\n",
        "\n",
        "In general, moderate departures from normality are of little concern in the fixed effects analysis of variance. Departures from normality usually cause both the true significance level and the power to differ slightly from the advertised values, with the power generally being lower. The random effects model that we will discuss in Section 3.5 is more severely affected by nonnormality.\n",
        "\n",
        "A very common defect that often shows up on normal probability plots is one residual that is very much larger than any of the others. Such a residual is often called an [outlier](https://en.wikipedia.org/wiki/Outlier). The\n",
        "presence of one or more outliers can seriously distort the analysis of variance, so when a potential outlier is located, careful investigation is called for.\n",
        "\n",
        "Several formal statistical procedures may be used for detecting outliers. A rough check for outliers may be made by examining the **standardized residuals**.\n",
        "\n",
        "$$d_{ij}=\\frac{e_{ij}}{\\sqrt{MS_E}}$$\n",
        "\n",
        "If the errors $\\epsilon_{ij}\\sim N(0,\\sigma^2)$, the standardized residuals $d_{ij}\\sim N(0,1)$. Thus, about 68 percent of the standardized residuals should fall within the\n",
        "limits 1, about 95 percent of them should fall within 2. A residual bigger than 3 or 4 standard deviations from zero is a potential outlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGHxi9BRhG6F",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.2 Plot of residuals in time sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpHX-CDYgylW",
        "colab_type": "text"
      },
      "source": [
        "Plotting the residuals in time order of data collection helps detect strong **correlation** between the residuals. A tendency to have runs of positive and negative residuals indicates a positive correlation. This would imply that the **independence assumption** on the errors has been violated. It is important to prevent the problem if possible when the data are collected. Proper randomization of the experiment is an important step in obtaining independence.\n",
        "\n",
        "Nonconstant variance is a potentially serious problem. If this assumption is violated, a plot of residuals versus time that exhibits more spread at one end than at the other. \n",
        "\n",
        "Table 3.6 displays the residuals and the time sequence of data collection for the click-through rate data. A plot of these residuals versus run order or time is shown in Figure 3.5. There is no reason to suspect any violation of independence or constant variance assumptions.\n",
        "\n",
        "![](https://i.ibb.co/M8MS1PM/plot-residuals.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6_UJZMfhG9u",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.3 Plot of residuals versus fitted values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi-xvcGGiazY",
        "colab_type": "text"
      },
      "source": [
        "If the assumption of homogeneity of variances is violated, the $F$ test is only slightly affected in the balanced (equal sample sizes in all treatments) fixed effects model. However, in unbalanced designs or in cases where one variance is very much larger than the others, the problem is more serious. Specifically, if the factor levels having the larger variances also have smaller sample sizes, the actual type I error rate is larger than anticipated. Conversely, if the factor levels with larger variances also have larger sample sizes, the significance levels are smaller than anticipated. This is a good reason for choosing equal sample sizes whenever possible. For random-effects models, unequal error variances can significantly disturb inferences on variance components even if balanced designs are used.\n",
        "\n",
        "The usual approach to dealing with nonconstant variance is to apply a **variance-stabilizing transformation** and then to run the analysis of variance on the transformed data. In this approach, one should note that the conclusions of the analysis of variance applied to the transformed populations.\n",
        "\n",
        "Considerable research has been devoted to the selection of an appropriate transformation. For example, if the observations follow the Poisson o lognormal distributions, the square root transformation or the logarithmic transformation are appropriate respectively. When there is no obvious transformation, the experimenter usually empirically seeks a transformation that equalizes the variance regardless of the value of the mean. In factorial experiments, which\n",
        "we introduce in Chapter 5, another approach is to select a transformation that minimizes the interaction mean square. Transformations made for inequality of variance also affect the form of the error distribution. In most cases, the transformation brings the error distribution closer to normal.\n",
        "\n",
        "- **Statistical tests for equality of variance**. Although residual plots are frequently used to diagnose inequality of variance, several statistical tests have also been proposed. These tests may be viewed as formal tests of the hypotheses\n",
        "$$H_o:\\sigma_1^2=\\sigma_2^2=...=\\sigma_a^2$$\n",
        "\n",
        "A widely used procedure is [Bartlett’s test](https://en.wikipedia.org/wiki/Bartlett%27s_test). The procedure involves computing a statistic whose sampling distribution is closely approximated by the chi-square distribution with $a-1$ degrees of freedom when the $a$ random samples are from independent normal populations.\n",
        "\n",
        "Therefore, we should reject $H_0$ when the test statistic $ \\chi_{o}^2>\\chi_{\\alpha,a-1}^2$\n",
        "where $\\chi_{\\alpha,a-1}^2$ is the upper percentage point of the chi-square distribution with $a-1$ degrees of freedom. The P-value approach to decision making could also be used.\n",
        "\n",
        "Bartlett’s test is very sensitive to the normality assumption. Consequently, when the\n",
        "validity of this assumption is doubtful, Bartlett’s test should not be used. Although other alternative procedures would be useful.\n",
        "\n",
        "The [modified Levene test](https://en.wikipedia.org/wiki/Levene%27s_test) is a very nice procedure that is robust to departures from normality. To test the hypothesis of equal variances in all treatments, the modified Levene test uses the absolute deviation of the observations $y_{ij}$ in each treatment from\n",
        "the treatment median, say, $ỹ_i$ . Denote these deviations by\n",
        "$$d_{ij}=|y_{ij}-ỹ_i|$$\n",
        "\n",
        "The modified Levene test then evaluates whether or not the means of these deviations are equal for all treatments. It turns out that if the mean deviations are equal, the variances of the observations in all treatments will be the same. The test statistic for Levene’s test is simply the usual ANOVA F statistic for testing equality of means applied to the absolute deviations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I93QTH8fhUDr",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.4 Plots of residuals versus other variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciS0_fphhdGX",
        "colab_type": "text"
      },
      "source": [
        "If data have been collected on any other variables that might affect the response, the residuals should be plotted against these variables.\n",
        "\n",
        "Patterns in such residual plots imply that the variable affects the response. This suggests that the variable should be either controlled more carefully in future experiments or included in the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6kSc99lhHA4",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Interpretation of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7_0g7UdiCx5",
        "colab_type": "text"
      },
      "source": [
        "After conducting the experiment, performing the statistical analysis, and investigating the assumptions, the experimenter is ready to draw practical conclusions about the problem. This might be done somewhat informally, perhaps by inspection of graphical displays such as the box plots and scatter diagram. However, in some cases, more formal techniques need to be applied. We will present some of these techniques in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thk3Bd78iC70",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.1 Comparisons Among Treatment Means\n",
        "\n",
        "Suppose that in conducting an analysis of variance for the fixed effects model the null hypothesis is rejected. Thus, there are differences between the treatment means but exactly which means differ is not specified. Sometimes in this situation, further comparisons and analysis among groups of treatment means may be useful. The $i$th treatment mean is defined as $\\mu_i=\\mu+\\tau_i$ and $\\mu_i$ is estimated by $\\overline{y}_{i.}$. Comparisons between treatment means are made in terms of either the treatment totals $y_{i.}$ or the treatment averages $\\overline{y}_{i.}$. The procedures for making these comparisons are usually called **multiple comparison methods**. In the next sections, we discuss methods for making comparisons among individual treatment means or groups of these means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwKZAArNk0bJ",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.2 Graphical Comparisons of Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYaMQcTKiC-q",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wb4fCcwk4N5",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.3 Contrasts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAGQW51ElrD3",
        "colab_type": "text"
      },
      "source": [
        "Many multiple comparison methods use the idea of a **contrast**. In general, a contrast is a linear combination of parameters of the form\n",
        "$$\\sum_{i=1}^{a}c_i\\mu_i$$\n",
        "where the contrast constants $c_1,c_2, . . . ,c_a$ sum to zero; that is $\\sum_{i=1}^{a}c_i=0$. The most general constrast is\n",
        "\u000e\u0005\u0005$$H_0:\\sum_{i=1}^{a}c_i\\mu_i=0$$\n",
        "\u000e\u0005\u0005$$H_1:\\sum_{i=1}^{a}c_i\\mu_i\\neq 0$$\n",
        "\n",
        "Thus if we would like to test the hypothesis\n",
        "\n",
        "\u000e\u0005\u0005$$H_0:\\mu_3=\\mu_4$$\n",
        "\u000e\u0005\u0005$$H_1:\\mu_3\\neq \\mu_4$$\n",
        "\n",
        "or equivalently\n",
        "\n",
        "\u000e\u0005\u0005$$H_0:\\mu_3-\\mu_4=0$$\n",
        "\u000e\u0005\u0005$$H_1:\\mu_3-\\mu_4 \\neq 0$$\n",
        "\n",
        "is equivalent to\n",
        "\n",
        "\u000e\u0005\u0005$$H_0:\\sum_{i=1}^{a}c_i\\mu_i=0$$\n",
        "\u000e\u0005\u0005$$H_1:\\sum_{i=1}^{a}c_i\\mu_i\\neq 0$$\n",
        "\n",
        "with the contrast constants $c_3=1,c_4=-1, c_1,c_2,c_5,...,c_a=0$\n",
        "\n",
        "Testing hypotheses involving contrasts can be done in two basic ways. The first method uses a $t$-test. Write the contrast of interest in terms of the treatment averages\n",
        "$$C=\\sum_{i=1}^{a}c_i\\overline{y}_{i.}$$\n",
        "\n",
        "The variance of $C$ is $V(C)=\\frac{\\sigma^2}{n}\\sum_{i=1}^{a}c_i^2$\n",
        "when the sample sizes in each treatment are equal. If the null hypothesis is true, the ratio\n",
        "$$\\frac{\\sum_{i=1}^{a}c_i\\overline{y}_{i.}}{\\sqrt{\\frac{\\sigma^2}{n}\\sum_{i=1}^{a}c_i^2}}\\sim N(0,1)$$\n",
        "\n",
        "Now we would replace the unknown variance \u0002$\\sigma^2$ by its estimate, the mean square error $MS_E$ and use the statistic\n",
        "\n",
        "$$t_0=\\frac{\\sum_{i=1}^{a}c_i\\overline{y}_{i.}}{\\sqrt{\\frac{MS_E}{n}\\sum_{i=1}^{a}c_i^2}}\\sim t_{N-a}$$\n",
        "\n",
        "The null hypothesis would be rejected if $|t_0|>t_{\\frac{\\alpha}{2},N-a}$.\n",
        "\n",
        "The second approach uses an $F$ test. Now the square of a $t$ random variable with $v$ \u0016degrees of freedom is an F random variable with 1 numerator and $v$ denominator degrees of freedom. Therefore, we can obtain \n",
        "\n",
        "$$F_0=t_0^2 = \\frac{(\\sum_{i=1}^{a}c_i\\overline{y}_{i.})^2}{\\frac{MS_E}{n}\\sum_{i=1}^{a}c_i^2}=\\frac{MS_C}{MS_E}=\\frac{\\frac{SS_C}{1}}{SS_E}\\sim F_{1,N-a}$$\n",
        "\n",
        "where the single degree of freedom contrast sum of squares is\n",
        "$$SS_C=\\frac{(\\sum_{i=1}^{a}c_i\\overline{y}_{i.})^2}{\\frac{1}{n}\\sum_{i=1}^{a}c_i^2}$$\n",
        "\n",
        "The null hypothesis would be rejected if $F_0>F_{\\alpha,1,N-a}$. \n",
        "\n",
        "Instead of testing hypotheses about a contrast, we can construct a confidence interval.\n",
        "\n",
        "The $100(1-\\alpha\u0002)$ percent confidence interval on the contrast $\\sum_{i=1}^{a}c_i\\mu_i$ is\n",
        "\n",
        "\n",
        "$$\\sum_{i=1}^{a}c_i\\overline{y}_{i.}-t_{\\frac{\\alpha}{2},N-a}\\sqrt{\\frac{MS_E}{n}\\sum_{i=1}^{a}c_i^2}\\leq \\sum_{i=1}^{a}c_i\\mu_i \\leq \\sum_{i=1}^{a}c_i\\overline{y}_{i.}+t_{\\frac{\\alpha}{2},N-a}\\sqrt{\\frac{MS_E}{n}\\sum_{i=1}^{a}c_i^2}$$\n",
        "\n",
        "Note that we have used $MS_E$ to estimate $\\sigma^\u00022$. If the confidence interval includes zero, we would be unable to reject the null hypothesis.\n",
        "\n",
        "- **Standardized Contrast**. When more than one contrast is of interest, it is often useful to evaluate them on the same scale. One way to do this is to standardize the contrast so that it has variance \u0002$\\sigma^2$.\n",
        "If the contrast $\\sum_{i=1}^{a}c_i\\mu_ {i}$ is written in terms of treatment averages as $\\sum_{i=1}^{a}c_i\\overline{y}_{i.}$, dividing it by $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{a}c_i^2}$ will produce a standardized contrast with variance \u0002$\\sigma^2$. Effectively, then, the standardized contrast is\n",
        "$$\\sum_{i=1}^{a}c^{\\prime}_i\\mu_{i}$$ where \n",
        "$$c^{\\prime}_i=\\frac{c_i}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{a}c_i^2}}$$\n",
        "\n",
        "When the sample sizes in each treatment are different, some modifications are made in the above results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdJwvZuue7Kl",
        "colab_type": "text"
      },
      "source": [
        "A useful special case of these sort of contrasts are the **orthogonal contrasts**. Two contrasts with coefficients $c_i$ and $d_i$ are orthogonal if\n",
        "$$\\sum_{i=1}^{a}c_id_i=0$$\n",
        "or, for an unbalanced design, if\n",
        "$$\\sum_{i=1}^{a}n_ic_id_i=0$$\n",
        "\n",
        "For $a$ treatments, the set of $a-\u00021$ orthogonal contrasts partition the sum of squares due to treatments into $a-\u00021$ independent single-degree-of-freedom components. Thus, tests performed on orthogonal contrasts are independent.\n",
        "\n",
        "Usually, something in the nature of the experiment should suggest which comparisons will be of interest and thus choose the orthogonal contrast coefficients for the set of treatments. For example, if there are $a=\u00053$ treatments, with treatment 1 a control and treatments 2 and 3 actual levels of the factor of interest to the experimenter, appropriate orthogonal contrasts might be as follows: $c_i=-2,1,1, i=1,2,3$ (which compares the average effect of the factor with the control) and $d_i=0,1,-1$ (which compares the two levels of the factor of interest).\n",
        "\n",
        "Generally, the method of contrasts (or orthogonal contrasts) is useful for what are called **preplanned comparisons**. That is, the contrasts are specified before running the experiment and examining the data. The reason for this is that if comparisons are selected after examining the data, most experimenters would construct tests that correspond to large observed differences in means. These large differences could be the result of the presence of real effects, or they could be the result of random error. If experimenters consistently pick the largest differences to compare, they will inflate the type I error of the test because it is likely that, in an unusually high percentage of the comparisons selected, the observed differences will be the result of the error. Examining the data to select comparisons of potential interest is often called **data snooping**. The Scheffé method for all comparisons, discussed in the next section, permits data snooping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMTs035ciDBh",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.4 Scheffé’s method for comparing all contrasts\n",
        "\n",
        "In many situations, experimenters may not know in advance which contrasts they wish to compare, or they may be interested in more than $a-1\u0002$ possible comparisons. \n",
        "\n",
        "Scheffé (1953) has proposed a [Scheffé’s Method](https://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method) for comparing any possible contrasts between treatment means. In the Scheffé method, the type I error is at most $\\alpha$ for any of the possible comparisons.\n",
        "\n",
        "Suppose that a set of $m$ contrasts in the treatment means\n",
        "$$\\Gamma_u=c_{1u}\\mu_1+c_{1u}\\mu_1+...+c_{au}\\mu_a, u=1,...,m$$\n",
        "\n",
        "of interest have been determined. The corresponding contrast in the treatment averages $\\overline{y}_{i.}$ is\n",
        "$$C_u=c_{1u}\\overline{y}_{1.}+c_{1u}\\overline{y}_{2.}+...+c_{au}\\overline{y}_{a.}, u=1,...,m$$\n",
        "and the standard errorof this contrast is\n",
        "$$S_{C_u}=\\sqrt{MS_E \\sum_{i=1}^{a}(\\frac{c_{iu}^2}{n_i})}$$\n",
        "\n",
        "where $n_i$ is the number of observations in the $i$th treatment. It can be shown that the critical value against which $C_u$ should be compared is\n",
        "\n",
        "$$S_{\\alpha,u}=S_{C_u}\\sqrt{(1-a)F_{\\alpha,a-1,N-a}}$$\n",
        "\n",
        "To test the hypothesis that the contrast \u000e$\\Gamma_u$ differs significantly from zero, refer $C_u$ to the critical value. If $|C_u|>S_{\\alpha,u}$, the hypothesis that the contrast \u000e$\\Gamma_u$ equals zero is rejected.\n",
        "\n",
        "\n",
        "\n",
        "The Scheffé procedure can also be used to form confidence intervals for all possible contrasts among treatment means. The resulting intervals\n",
        "$$C_u-S_{\\alpha,u} \\leq \\Gamma_u \\leq C_u+S_{\\alpha,u}$$\n",
        "are **simultaneous confidence intervals** in that the probability that all of them are simultaneously true is at least $1-\\alpha$\u0002.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAR_TSK3lD_U",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.4 Scheffé’s method for comparing all contrasts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-xCKTwulEDX",
        "colab_type": "text"
      },
      "source": [
        "In many situations, experimenters may not know in advance which contrasts they wish to compare, or they may be interested in more than $a-1\u0002$ possible comparisons. \n",
        "\n",
        "Scheffé (1953) has proposed a [Scheffé’s Method](https://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method) for comparing any possible contrasts between treatment means. In the Scheffé method, the type I error is at most $\\alpha$ for any of the possible comparisons.\n",
        "\n",
        "Suppose that a set of $m$ contrasts in the treatment means\n",
        "$$\\Gamma_u=c_{1u}\\mu_1+c_{1u}\\mu_1+...+c_{au}\\mu_a, u=1,...,m$$\n",
        "\n",
        "of interest have been determined. The corresponding contrast in the treatment averages $\\overline{y}_{i.}$ is\n",
        "$$C_u=c_{1u}\\overline{y}_{1.}+c_{1u}\\overline{y}_{2.}+...+c_{au}\\overline{y}_{a.}, u=1,...,m$$\n",
        "and the standard errorof this contrast is\n",
        "$$S_{C_u}=\\sqrt{MS_E \\sum_{i=1}^{a}(\\frac{c_{iu}^2}{n_i})}$$\n",
        "\n",
        "where $n_i$ is the number of observations in the $i$th treatment. It can be shown that the critical value against which $C_u$ should be compared is\n",
        "\n",
        "$$S_{\\alpha,u}=S_{C_u}\\sqrt{(1-a)F_{\\alpha,a-1,N-a}}$$\n",
        "\n",
        "To test the hypothesis that the contrast \u000e$\\Gamma_u$ differs significantly from zero, refer $C_u$ to the critical value. If $|C_u|>S_{\\alpha,u}$, the hypothesis that the contrast \u000e$\\Gamma_u$ equals zero is rejected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVoVE8Zy76Z",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.5 Comparing pairs of treatment means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMIKz5YclEGP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "In many practical situations, we will wish to compare only pairs of means. Frequently, we can determine which means differ by testing the differences between all pairs of treatment means. Thus, we are interested in contrasts of the form \u000e\u0005\u0005$H_0:\\mu_i=\\mu_j$ for all $i\u0002\\neq j$. We now consider methods specifically designed for pairwise comparisons between all $a$ population means.\n",
        "\n",
        "\n",
        "Suppose that we are interested in comparing all pairs of $a$ treatment means. We now present two popular methods for making such comparisons.\n",
        "\n",
        "The [Tukey’s Test](https://en.wikipedia.org/wiki/Tukey%27s_range_test) is a single-step multiple comparison procedure and statistical test that can be used to find means that are significantly different from each other. This procedure can also be used to construct confidence intervals on the differences in all pairs of means.\n",
        "\n",
        "Suppose that, following an ANOVA in which we have rejected the null hypothesis of equal treatment means, we wish to test all pairwise mean comparisons:\n",
        "\u000e\u0005\u0005$$H_0:\\mu_i=\\mu_j$$\n",
        "\u000e\u0005\u0005$$H_1:\\mu_i\\neq \\mu_j$$\n",
        "\n",
        "for all $i\u0002\\neq j$. \n",
        "\n",
        "Tukey’s procedure makes use of the distribution of the studentized range statistic.\n",
        "$$q=\\frac{\\overline{y}_{max}-\\overline{y}_{min}}{\\sqrt{\\frac{MS_E}{n}}}$$\n",
        "\n",
        "where $\\overline{y}_{max}$ and $\\overline{y}_{min}$ are the largest and smallest sample means, respectively, out of a group of a sample means.\n",
        "\n",
        "Tukey’s test declares two means significantly different if \n",
        "\n",
        "$$|\\overline{y}_{i.}-\\overline{y}_{j.}|>T_{\\alpha}=q_{\\alpha}(a,f)\\sqrt{\\frac{MS_E}{n}}$$\n",
        "\n",
        "where $f$ is the number of degrees of freedom associated with the $MS_E$. The distribution of $q_{\\alpha}$ has been tabulated and appears in many textbooks on statistics. Equivalently, we could construct a set of confidence intervals for all pairs of means as follows:\n",
        "$$\\overline{y}_{i.}-\\overline{y}_{j.}-q_{\\alpha}(a,f)\\sqrt{\\frac{MS_E}{n}}\\leq \\mu_i-\\mu_j \\leq \\overline{y}_{i.}-\\overline{y}_{j.}+q_{\\alpha}(a,f)\\sqrt{\\frac{MS_E}{n}}$$\n",
        "\n",
        "Also, note that the sample sizes must be equal when using the studentized range approach. It is possible to work with unequal sample sizes. In this case, one has to calculate the estimated standard deviation for each pairwise comparison as formalized by Clyde Kramer in 1956, so the procedure for unequal sample sizes is sometimes referred to as the **Tukey–Kramer method** which is as follows: \n",
        "\n",
        "$$T_{\\alpha}=q_{\\alpha}(a,f)\\sqrt{MS_E(\\frac{1}{n_i}+\\frac{1}{n_j})}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX6Y_U7EwYSr",
        "colab_type": "text"
      },
      "source": [
        "The **Fisher Least Significant Difference (LSD)** is used for comparing all pairs of means controls the error rate $\\alpha$ for each pairwise comparison but does not control the experimentwise or family error rate. This procedure uses the $t$ statistic for testing $H_0:\\mu_i=\\mu_j$\n",
        "\n",
        "$$t_{0}=\\frac{\\overline{y}_{i.}-\\overline{y}_{j.}}{\\sqrt{MS_E(\\frac{1}{n_i}+\\frac{1}{n_j})}}$$\n",
        "\n",
        "the pair of means $\\mu_i$ and $\\mu_j$ would be declared significantly different if\n",
        "\n",
        "$$|\\overline{y}_{i.}-\\overline{y}_{j.}|>LSD=t_{\\frac{\\alpha}{2}N-a}\\sqrt{MS_E(\\frac{1}{n_i}+\\frac{1}{n_j})}$$\n",
        "\n",
        "The quantity LSD is called the **least significant difference**\n",
        "\n",
        "Note that the overall $\\alpha$ risk may be considerably inflated using this method. Specifically, as the number of treatments $a$ gets larger, the experimentwise or family type I error rate (the ratio of the number of experiments in which at least one type I error is made to the total number of experiments) becomes large.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78eUav8_vVCD",
        "colab_type": "text"
      },
      "source": [
        "- Which one of these procedures should I use? Unfortunately, there is no clear answer to this question, and professional statisticians often disagree over the utility of the various procedures. Carmer and Swanson report that the LSD method is a very effective test for detecting true differences in means if it is applied only after the $F$ test in the ANOVA is significant at 5 percent. However, this method does not contain the experimentwise error rate. Because the Tukey method does control the overall error rate, many statisticians prefer to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYmRH_XxaJk",
        "colab_type": "text"
      },
      "source": [
        "### 4.4.6 Comparing treatment means with a control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiJmQWIfyS9H",
        "colab_type": "text"
      },
      "source": [
        "In many experiments, one of the treatments is a control, and the analyst is interested in comparing each of the other $a-\u00021$ treatment means with the control. Thus, only $a-\u00021$ comparisons are to be made. A procedure for making these comparisons has been developed by Dunnett (1964). Suppose that treatment $a$ is the control and we wish to test the hypotheses\n",
        "\n",
        "$$H_0:\\mu_i=\\mu_a$$\n",
        "\u000e\u0005\u0005$$H_1:\\mu_i\\neq \\mu_a$$\n",
        "\n",
        "for $i=1,...,a-1$. Dunnett’s procedure is a modification of the usual t-test. For each hypothesis, we compute the observed differences in the sample means\n",
        "\n",
        "$$|\\overline{y}_{i.}-\\overline{y}_{a.}|, i=1,...,a-1$$\n",
        "\n",
        "The null hypothesis $H0$ is rejected using a type I error rate $\\alpha$ if\n",
        "$$|\\overline{y}_{i.}-\\overline{y}_{a.}|> d_{\\alpha}(a-1,f)\\sqrt{MS_E(\\frac{1}{n_i}+\\frac{1}{n_a})}$$\n",
        "\n",
        "where the constant $d_{\\alpha}(a-1,f)$ is given in a specific Table. Note that $\\alpha$ is the **joint significance level** associated with all $a\u0002-1$ tests.\n",
        "\n",
        "When comparing treatments with a control, it is a good idea to use more observations for the control treatment ($n_a$) than for the other treatments ($n$), assuming equal numbers of observations for the remaining $a-\u00021$ treatments. The ratio $\\frac{n_a}{n}$ should be chosen to be approximately equal to the square root of the total number of treatments. That is, choose $\\frac{n_a}{n}=\\sqrt{a}$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFWJM975iDD-",
        "colab_type": "text"
      },
      "source": [
        "## 3.5 The random effects model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0E0lyhWyQDR",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.1 A single random factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_0t9pCyQIk",
        "colab_type": "text"
      },
      "source": [
        "An experimenter is frequently interested in a factor that has a large number of possible levels. If the experimenter randomly selects $a$ of these levels from the population of factor levels, then we say that the factor is random. Because the levels of the factor actually used in the experiment were chosen randomly, inferences are made about the entire population of factor levels. We assume that the population of factor levels is either of infinite size or is large enough to be considered infinite. Situations in which the population of factor levels is small enough to employ a finite population approach are not encountered frequently. The linear statistical model is\n",
        "\n",
        "$$y_{ij} = \\mu_{i} + \\epsilon_{ij}; i=1,2,...,a; j=1,...,n$$    \n",
        "\n",
        "where both the treatment effects \u000f$\\tau_i$ and \b$\\epsilon_{ij}$ are random variables. We will assume that the treatment effects \u000f\u000f$\\tau_i$  are $NID (0,\\sigma_{\\tau}^2)$ random variables and that the errors are $NID (0,\\sigma^2)$, random variables, and that the \u000f$\\tau_i$ and \b$\\epsilon_{ij}$ are independent. Because \u000f$\\tau_i$ is independent of \b$\\epsilon_{ij}$, the variance of any observation is\n",
        "$$V(y_{ij})=\\sigma_{\\tau}^2+\\sigma^2$$\n",
        "\n",
        "The variances $\\sigma_{\\tau}^2$ and \u0002$\\sigma^2$ are called **variance components**, and the model is called the **components of variance** or **random effects model**. The observations in the random-effects model are normally distributed because they are linear combinations of the two normally and independently distributed random variables \u000f$\\tau_i$ and \b$\\epsilon_{ij}$.  However, unlike the case of the fixed effects in which all of the observations $y_{ij}$ are independent, in the random model, the observations $y_{ij}$ are only independent if they come from different factor levels. Specifically, we can show that the covariance of any two observations is\n",
        "\n",
        "$$Cov(y_{ij},y_{ij^{\\prime}})=\\sigma_{\\tau}^2 \\text{ if }i\\neq j^{\\prime}$$\n",
        "$$Cov(y_{ij},y_{i^{\\prime}j^{\\prime}})=\\sigma_{\\tau}^2 \\text{ if }i\\neq i^{\\prime}$$\n",
        "\n",
        "Note that the observations within a specific factor level all have the same covariance because before the experiment is conducted, we expect the observations at that factor level to be similar because they all have the same random component. Once the experiment has been conducted, we can assume that all observations can be assumed to be independent, because the parameter $\\tau_i$ has been determined and the observations in that treatment differ only because of random error.\n",
        "\n",
        "We can express the covariance structure of the observations in the single-factor random-effects model through the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix) of the observations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI4LwmYoT32z",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.2 Analysis of variance for the random model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQsJlOoTiRmq",
        "colab_type": "text"
      },
      "source": [
        "The basic ANOVA sum of squares identity\n",
        "$$\n",
        "SS_T = SS_{Treatments} + SS_E\n",
        "$$\n",
        "is still valid. That is, we partition the total variability in the observations into a component that measures the variation between treatments ($SS_{Treatments}$)  and a component that measures the variation within treatments ($SS_E$). Testing hypotheses about individual treatment effects is not very meaningful because they were selected randomly, we are more interested in the population of treatments, so we test hypotheses about the variance component $\\sigma_{\\tau}\n",
        "^2$\n",
        "\n",
        "$$\n",
        "H_0: \\sigma_{\\tau}^2=0\\\\\n",
        "H_1: \\sigma_{\\tau}^2>0\n",
        "$$\n",
        "\n",
        "If $\\sigma_{\\tau}^2=0$, all treatments are identical; but if \\sigma_{\\tau}^2\\neq 0, variability exists between treatments. \n",
        "\n",
        "As before, $\\frac{SS_E}{\\sigma^2}\\sim \\chi_{N-a}$ and, under the null hypothesis,$\\frac{SS_Treatments}{\\sigma^2}\\sim \\chi_{a-1}$. Both random variables are independent. Thus, under the null hypothesis, the ratio\n",
        "$$\n",
        "F_0=\\frac{\\frac{SS_{Treatments}}{a-1}}{\\frac{SS_E}{N-a}}=\\frac{MS_{Treatments}}{MS_E}\\sim F_{a-1,N-a}\n",
        "$$\n",
        "\n",
        "Specifically  $E(MS_{Treatments})=\\sigma^2+n\\sigma_{\\tau}^2$ and $E(MS_{E})=\\sigma^2$.\n",
        "\n",
        "From the expected mean squares, we see that under $H_0$ both the numerator and denominator of the test statistic ($F_0$) are unbiased estimators of \u0002$\\sigma^2$, whereas under $H_1$ the expected value of the numerator is greater than the expected value of the denominator. Therefore, we should reject $H_0$ for values of $F_0$ that are too large. This implies an upper-tail, one-tail critical region, so we reject $H_0$ if $F_0>F_{\\alpha, a-1, N-a}$.The computational procedure and ANOVA for the random-effects model are identical to those for the case of the fixed effects. The conclusions, however, are quite different because they apply to the entire population of treatments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc7LoFH9ZTLP",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.3 Estimating the model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdSkbJCQZZ7a",
        "colab_type": "text"
      },
      "source": [
        "We are usually interested in estimating the variance components (\u0002$\\sigma$ and $\\sigma_{\\tau}$) in the model. One very simple procedure that we can use to estimate $\\sigma$ and $\\sigma_{\\tau}$ is called the **analysis of variance method** because it makes use of the lines in the analysis of variance table. The procedure consists of equating the expected mean squares to their observed values in the ANOVA table and solving for the variance components. In equating  observed and expected mean squares in the single-factor random-effects model, we obtain\n",
        "\n",
        "$$MS_{Treatments}=\u0002\\sigma^2+\\sigma_{\\tau}^2$$ and $$MS_E=\\sigma^2$$\n",
        "\n",
        "Therefore, the estimators of the variance components are\n",
        "$$\\hat{\\sigma}^2=MS_E$$\n",
        "and\n",
        "$$\\hat{\\sigma^2_{\\tau}} = \\frac{MS_{Treatments}-MS_{E}}{n}$$\n",
        "\n",
        "The analysis of variance method of variance component estimation is a **method of moments procedure**. It does not require the normality assumption. It does yield estimators of \u0002$\\sigma$ and $\\sigma_{\\tau}$ that are best quadratic unbiased (i.e., of all unbiased quadratic functions of the observations, these estimators have minimum variance). There is a different method based on the maximum likelihood that can be used to estimate the variance components that will be introduced later.\n",
        "\n",
        "Occasionally, the analysis of variance method produces a negative estimate of a variance component. Variance components are by definition non-negative, so a negative estimate of a variance component is viewed with some concern. One course of action is to accept the estimate and use it as evidence that the true value of the variance component is zero, assuming that sampling variation led to the negative estimate. This has intuitive appeal, but it suffers from some theoretical difficulties. For instance, using zero in place of the negative estimate can disturb the statistical properties of other estimates. Another alternative is to reestimate the negative variance component using a method that always yields nonnegative estimates. Still another alternative is to consider the negative estimate as evidence that the assumed linear model is incorrect and reexamine the problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTg4nS8BRMyN",
        "colab_type": "text"
      },
      "source": [
        "**Maximum Likelihood Estimation of the Variance Components**. Earlier in this section, we presented the analysis of variance method of variance component estimation. This method is relatively straightforward to apply and makes use of familiar quantities—the mean squares in the analysis of variance table. However, the method has some disadvantages. As we pointed out previously, it is a method of moments estimator, a technique that mathematical statisticians generally do not prefer to use for parameter estimation because it often results in parameter estimates that do not have good statistical properties. One obvious problem is that it does not always lead to an easy way to construct confidence intervals on the variance components of interest. For example, in the single-factor random model, there is not a simple way to construct confidence intervals on $\\sigma_{\\tau}$, which is certainly a parameter of primary interest to the experimenter. The preferred parameter estimation technique is called the [method of maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation). The implementation of this method can be somewhat involved, particularly for an experimental design model, but it has been incorporated in some modern computer software packages that support designed experiments.\n",
        "\n",
        "The general idea of this method can be illustrated very easily. Suppose that $x$ is a random variable with probability distribution $f(x,\\theta)$, where $\\theta$ is an unknown parameter. Let $x_1,x_2, . . . ,x_n$ be a random sample of $n$ observations. The joint probability distribution of the sample is $\\prod_{i=1}^{n}f(x_i,\\theta)$. The **likelihood function** is just this joint probability distribution with the sample observations consider fixed and the parameter $\\theta$ unknown. Note that the likelihood function\n",
        "$$L(x_1,...,x_n;\\theta)=\\prod_{i=1}^{n}f(x_i,\\theta)$$\n",
        "\n",
        "is now a function of only the unknown parameter $\\theta$. The **maximum likelihood estimator** of $\\theta$ is the value of $\\theta$ that maximizes the likelihood function $L(x_1,...,x_n;\\theta)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHuDH7sgiRpw",
        "colab_type": "text"
      },
      "source": [
        "## 3.6 Non parametric methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePlvGbXZhHDs",
        "colab_type": "text"
      },
      "source": [
        "### 3.6.1 The Kruskal–Wallis Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DAmVzVBzSLN",
        "colab_type": "text"
      },
      "source": [
        "In situations where the normality assumption is unjustified, the experimenter may wish to use an alternative procedure to the F test analysis of variance that does not depend on this assumption. Such a procedure has been developed by Kruskal and Wallis (1952). The Kruskal–Wallis test is used to test the null hypothesis that the $a$ treatments are identical against the alternative hypothesis that some of the treatments generate observations that are larger than others. Because the procedure is designed to be sensitive for testing differences in means, it is sometimes convenient to think of the  Kruskal–Wallis test as a test for equality of treatment means. The Kruskal–Wallis test is a **nonparametric alternative** to the usual analysis of variance.\n",
        "\n",
        "To perform a Kruskal–Wallis test, first rank the observations $y_{ij}$ in ascending order and replace each observation by its rank, say $R_{ij}$, with the smallest observation having rank 1. In the case of ties (observations having the same value), assign the average rank to each of the tied observations. Let $R_{i.}$ be the sum of the ranks in the $i$th treatment. The test statistic is\n",
        "$$H=\\frac{1}{S^2}[\\sum_{i=1}^{a}\\frac{R_{i.}}{n_i}-\\frac{N(N+1)^2}{4}]$$\n",
        "\n",
        "where $n_i$ is the number of observations in the $i$th treatment, $N$ is the total number of observations, and $S^2$ is just the variance of the ranks.\n",
        "\n",
        "$H$ is distributed approximately $\\chi_{a-1}$ under the null hypothesis. Therefore, if \n",
        "$$H>\\chi_{\\alpha,a-1}$$ \n",
        "the null hypothesis is rejected. The P-value approach could also be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8KWrpXVe4Iw",
        "colab_type": "text"
      },
      "source": [
        "Resources:\n",
        "\n",
        "- Douglas C. Montgomery Design and Analysis of Experiments Wiley"
      ]
    }
  ]
}