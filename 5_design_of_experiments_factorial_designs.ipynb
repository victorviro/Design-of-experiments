{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_design_of_experiments_factorial_designs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOV9gxe/p8zcHElJk0ble/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/design_of_experiments/blob/master/5_design_of_experiments_factorial_designs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHYBY5McpaXF",
        "colab_type": "text"
      },
      "source": [
        "# 5. Factorial designs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWxR6nwBYdJd",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Basic principles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLaq3JeDYdOO",
        "colab_type": "text"
      },
      "source": [
        "Many experiments involve the study of the effects of two or more factors. In general, factorial designs are most efficient for this type of experiment. By a factorial design, we mean that in each complete trial or replicate of the experiment all possible combinations of the levels of the factors are investigated. For example, if there are $a$ levels of factor $A$ and $b$ levels of factor $B$, each replicate contains all $ab$ treatment combinations. When factors are arranged in a factorial design, they are often said to be **crossed**.\n",
        "\n",
        "The effect of a factor is defined to be the change in response produced by a change in the level of the factor. This is frequently called a **main effect** because it refers to the primary factors of interest in the experiment.\n",
        "\n",
        "\n",
        "For example, consider the simple experiment in Figure 5.1. This is a two-factor factorial experiment with both design factors at two levels. We have called these levels “low” and “high” and denoted them “-” and “+” respectively. The main effect of factor $A$ in this two-level design can be thought of as the difference between the average response at the low level of $A$ and the average response at the high level of $A$. Numerically, this is\n",
        "$$A=\\frac{40+52}{2}-\\frac{20+30}{2}=21$$\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://i.ibb.co/THP2xjN/fd-1.png)\n",
        "\n",
        "\n",
        "That is, increasing factor $A$ from the low level to the high level causes an **average response increase** of 21 units. Similarly, the main effect of B is\n",
        "\n",
        "$$A=\\frac{30+52}{2}-\\frac{20+40}{2}=11$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2YN8aU2YdTO",
        "colab_type": "text"
      },
      "source": [
        "In some experiments, we may find that the difference in response between the levels of one factor is not the same at all levels of the other factors. When this occurs, there is an **interaction** between the factors.\n",
        "\n",
        "For example, consider the two-factor factorial experiment\n",
        "shown in Figure 5.2. At the low level of factor $B$ (or $B^{-}$), the $A$ effect is\n",
        "$$A=50-20=30$$\n",
        "\n",
        "and at the high level of factor $B$ (or $B^+$ ), the $A$ effect is\n",
        "$$A=12-40=-28$$\n",
        "\n",
        "Because the effect of $A$ depends on the level chosen for factor $B$, we see that there is an interaction between $A$ and $B$. The magnitude of the interaction effect is the average difference in these two $A$ effects, or $AB=\\frac{(-28-30)}{2}=-29$. The interaction is large in this experiment.\n",
        "\n",
        "These ideas may be illustrated graphically. Figure 5.3 plots the response data in Figure 5.1 against factor $A$ for both levels of factor $B$. Note that the $B^-$ and $B^+$ lines are approximately parallel, indicating a lack of interaction between factors $A$ and $B$. Similarly, Figure 5.4 plots the response data in Figure 5.2. Here we see that the $B^-$ and $B^+$ lines are not parallel.\n",
        "This indicates an interaction between factors $A$ and $B$. \n",
        "\n",
        "![alt text](https://i.ibb.co/80B7HwZ/fd-2.png)\n",
        "\n",
        "Two-factor interaction graphs such as these are frequently very useful in interpreting significant interactions and in reporting results to nonstatistically trained personnel. However, they should not be utilized as the sole technique of data analysis because their interpretation is subjective and their appearance is often\n",
        "misleading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX2oVNfwYdYU",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 Adventages of factorials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJBZoei0Ydas",
        "colab_type": "text"
      },
      "source": [
        "They are more efficient than one-factor-at-a-time experiments. Furthermore, a factorial design is necessary when interactions may be present to avoid misleading conclusions. Finally, factorial designs allow the effects of a factor to be estimated at several levels of the other factors, yielding conclusions that are valid over a range of experimental conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg7d2uNepabx",
        "colab_type": "text"
      },
      "source": [
        "## 5.3 The two-factor factorial design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsJzviWUpad0",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.1 An example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ44-C9ytqoy",
        "colab_type": "text"
      },
      "source": [
        "The simplest types of factorial designs involve only two factors or sets of treatments. There are $a$ levels of factor $A$ and $b$ levels of factor $B$, and these are arranged in a factorial design;\n",
        "that is, each replicate of the experiment contains all $ab$ treatment combinations. In general, there are $n$ replicates.\n",
        "\n",
        "As an example of a factorial design involving two factors, an engineer is designing a battery for use in a device that will be subjected to some extreme variations in temperature. The only design parameter that he can select at this point is the plate material for the battery, and he has three possible choices. When the device is manufactured and is shipped to the field, the engineer has no control over the temperature extremes that the device will encounter, and he knows from experience that temperature will probably affect the effective battery life. However, the temperature can be controlled in the product development laboratory for a test.\n",
        "\n",
        "The engineer decides to test all three plate materials at three temperature levels—$15, 70$, and $125°F$—because these temperature levels are consistent with the product end-use\n",
        "environment. Because there are two factors at three levels, this design is sometimes called a $3^2$ **factorial design**. Four batteries are tested at each combination of plate material and temperature, and all $36$ tests are run in random order. The experiment and the resulting observed battery life data are given in Table 5.1.\n",
        "\n",
        "![alt text](https://i.ibb.co/gRX4Wzv/fd-ex.png)\n",
        "\n",
        "In this problem the engineer wants to answer the following questions:\n",
        "-  What effects do material type and temperature have on the life of the battery?\n",
        "-  Is there a choice of material that would give uniformly long life regardless of *temperature*?\n",
        "\n",
        "\n",
        "This last question is particularly important. It may be possible to find a material alternative that is not greatly affected by temperature. If this is so, the engineer can make the battery\n",
        "**robust** to temperature variation in the field. This is an example of using statistical experimental design for robust product design, a very important engineering problem.\n",
        "\n",
        "This design is a specific example of the general case of a two-factor factorial. To pass to the general case, let $y_{ijk}$ be the observed response when factor $A$ is at the $i$th level ($i=1,\n",
        "2,...,a$) and factor $B$ is at the $j$th level ($j=1,2,...,b$) for the $k$th replicate ($k=1,2,...,n$).\n",
        "\n",
        "In general, a two-factor factorial experiment will appear as in Table 5.2. The order in which the $abn$ observations are taken is selected at random so that this design is a **completely randomized design**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHS9lC1btqsG",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.2 The fixed effects model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIJzwuT0tqvF",
        "colab_type": "text"
      },
      "source": [
        "The observations in a factorial experiment can be described by a model. There are several ways to write the model for a factorial experiment. The **effects model** is\n",
        "$$y_{ijk} = \\mu+\\tau_i + \\beta_j+ (\\tau\\beta)_{ij} +\\epsilon_{ijk} \\mspace{36mu} i=1,...,a \\mspace{12mu} j=1,...,b \\mspace{12mu} k=1,2,...,n$$ \n",
        "\n",
        "where $\\mu$\u0005 is the overall mean effect, \u000f$\\tau_i$ is the effect of the $i$th level of the row factor A, \u0003$\\beta_j$ is the effect of the $j$th level of column factor B, $(\u000f\\tau\\beta\u0003)_{ij}$ is the effect of the interaction between\n",
        "\u000f$\\tau_i$ and \u0003$\\beta_j$ , and \b$\\epsilon_{ijk}$ is a random error component. Both factors are assumed to be **fixed**, and the treatment effects are defined as deviations from the overall mean, so \n",
        "$$\\sum_{i=1}^{a}\\tau_i= \\sum_{j=1}^{b}\\beta_j=0$$\n",
        "\n",
        "Similarly, the interaction effects are fixed and are defined such that \n",
        "$$\\sum_{i=1}^{a}(\\tau\\beta)_{ij}= \\sum_{j=1}^{b}(\\tau\\beta)_{ij}=0$$\n",
        "\n",
        "Because there are $n$ replicates of the experiment, there are $abn$ total\n",
        "observations.\n",
        "\n",
        "![alt text](https://i.ibb.co/VWqSwCM/two-f-d.png)\n",
        "\n",
        "We could also use a regression model. Regression models are particularly useful when one or more of the factors in the experiment are quantitative. Throughout most of this chapter, we will use the effects model.\n",
        "\n",
        "\n",
        "In the two-factor factorial, both row and column factors (or treatments), A and B, are of equal interest. Specifically, we are interested in **testing hypotheses** about the equality of row treatment effects, say\n",
        "\n",
        "\n",
        "$$\n",
        "H_0: \\tau_1=\\tau_2=...=\\tau_a=0\\\\\n",
        "H_1: \\tau_i\\neq0 \\text{ for  at  least  one }i\n",
        "$$\n",
        "\n",
        "and the equality of column treatment effects, say\n",
        "\n",
        "$$\n",
        "H_0: \\beta_1=\\beta_2=...=\\beta_b=0\\\\\n",
        "H_1: \\beta_j\\neq0 \\text{ for  at  least  one }j\n",
        "$$\n",
        "\n",
        "We are also interested in determining whether row and column treatments *interact*. Thus, we also wish to test\n",
        "\n",
        "$$\n",
        "H_0: (\\tau\\beta)_{ij}=0 \\text{ for all }ij\\\\\n",
        "H_1: (\\tau\\beta)_{ij}\\neq0 \\text{ for  at  least  one pair }ij\n",
        "$$\n",
        "\n",
        "We now discuss how these hypotheses are tested using a **two-factor analysis of variance**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vVyWbi8uAGf",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.3 Statistical analysis of the fixed effects model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQmtEO6ytqxk",
        "colab_type": "text"
      },
      "source": [
        "Let $y_{i..}$ denote the total of all observations under the $i$th level of factor A, $y_{.j.}$ denote the total of all observations under the $j$th level of factor B, $y_{ij.}$ denote the total of all observations in the jth cell, and y...denote the grand total of all the observations. Define     ,,, and as thecorresponding row, column, cell, and grand averages. Expressed mathematically,\n",
        "\n",
        "$$y_{i..}=\\sum_{j=1}^{b}\\sum_{k=1}^{n}y_{ijk} \\mspace{36mu} \\overline{y}_{i..}=\\frac{y_{i..}}{bn} \\mspace{36mu} i=1,...,a$$\n",
        "\n",
        "$$y_{.j.}=\\sum_{i=1}^{a}\\sum_{k=1}^{n}y_{ijk} \\mspace{36mu} \\overline{y}_{.j.}=\\frac{y_{.j.}}{an} \\mspace{36mu} j=1,...,b$$\n",
        "\n",
        "$$y_{ij.}=\\sum_{k=1}^{n}y_{ijk} \\mspace{36mu} \\overline{y}_{ij.}=\\frac{y_{ij.}}{n} \\mspace{36mu} i=1,...,a, j=1,...,b$$\n",
        "\n",
        "$$y_{...}=\\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n}y_{ijk} \\mspace{36mu} \\overline{y}_{...}=\\frac{y_{...}}{abn} \\mspace{36mu} i=1,...,a, j=1,...,b$$\n",
        "\n",
        "The **total corrected sum of squares** may be written as\n",
        "\n",
        "$$\\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n}(y_{ijk}-\\overline{y}_{...})^2=bn\\sum_{i=1}^{a}(\\overline{y}_{i..}-\\overline{y}_{...})^2+an\\sum_{j=1}^{b}(\\overline{y}_{.j.}-\\overline{y}_{...})^2+n\\sum_{i=1}^{a}\\sum_{j=1}^{b}(\\overline{y}_{ij.}-\\overline{y}_{i..}-\\overline{y}_{.j.}-\\overline{y}_{...})^2+\\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{k=1}^{n}(y_{ijk}-\\overline{y}_{ij.})^2$$\n",
        "\n",
        "Notice that the total sum of squares has been partitioned into a sum of squares due to “rows,” or factor A, ($SS_A$); a sum of squares due to “columns,” or factor B, ($SS_B$); a sum of squares due to the interaction between A and B, ($SS_{AB}$); and a sum of squares due to error, ($SS_E$). This is the fundamental ANOVA equation for the two-factor factorial. From the last component on the right-hand side of the equation, we see that there must be at least two replicates ($n\\geq2$) to obtain an error sum of squares.\n",
        "We may write the fundamental ANOVA equation for the two-factor factorial symbolically as\n",
        "$$SS_T = SS_{A} + SS_{B} + SS_{AB} + SS_E$$\n",
        "\n",
        "The number of degrees of freedom associated with each sum of squares is\n",
        "\n",
        "![alt text](https://i.ibb.co/mywgG4y/d-o-f.png)\n",
        "\n",
        "Each sum of squares divided by its degrees of freedom is a **mean square**. The expected values of the mean squares are\n",
        "$$E(MS_A)=E(\\frac{SS_A}{a-1})=\\sigma^2+\\frac{bn\\sum_{i=1}^{a}\\tau_i^2}{a-1}$$\n",
        "$$E(MS_B)=E(\\frac{SS_B}{b-1})=\\sigma^2+\\frac{an\\sum_{j=1}^{b}\\beta_j^2}{b-1}$$\n",
        "\n",
        "$$E(MS_{AB})=E(\\frac{SS_{A}}{(a-1)(b-1)})=\\sigma^2+\\frac{n\\sum_{i=1}^{a}\\sum_{j=1}^{b}(\\tau\\beta)_{ij}^2}{(a-1)(b-1)}$$\n",
        "\n",
        "$$E(MS_E)=E(\\frac{SS_E}{ab(n-1)})=\\sigma^2$$\n",
        "\n",
        "Notice that if the null hypotheses of no row treatment effects, no column treatment effects, and no interaction are true, then $MS_A$, $MS_B$, $MS_{AB}$, and $MS_E$ all estimate $\\sigma^2$ . However, if there are differences between row treatment effects, say, then $MS_A$ will be larger than $MS_E$ .\n",
        "Similarly, if there are column treatment effects or interaction present, then the corresponding mean squares will be larger than MS E . Therefore, to test the significance of both main effects and their interaction, simply divide the corresponding mean square by the error mean square. Large values of this ratio imply that the data do not support the null hypothesis.\n",
        "\n",
        "If we assume that the model is adequate and that the error terms $\\epsilon_{ijk}$ are normally and independently distributed with constant variance $\\sigma^2$, then each of the ratios of mean squares $\\frac{MS_A}{MS_E}$, $\\frac{MS_B}{MS_E}$, $\\frac{MS_{AB}}{MS_E}$ is distributed as $F$ with $a-1$, $b-1$, and $(a-1)(b-1)$ numerator degrees of freedom, respectively, and $ab(n-1)$ denominator degrees of freedom, and the critical region would be the upper tail of the $F$\n",
        "distribution. The test procedure is usually summarized in an analysis of variance table, as shown in Table 5.3.\n",
        "\n",
        "\n",
        "![alt text](https://i.ibb.co/NL0DkB3/ttt.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X1In111CIKX",
        "colab_type": "text"
      },
      "source": [
        "**Example**: Table 5.4 presents the effective life (in hours) observed in the battery design example described in Section 5.3.1. The row and column totals are shown in the margins of the table, and the circled numbers are the cell totals.\n",
        "\n",
        "![alt text](https://i.ibb.co/JHG26b9/ex1.png)\n",
        "\n",
        "The ANOVA is shown in Table 5.5.\n",
        "\n",
        "![alt text](https://i.ibb.co/mbDkvzC/ex2.png)\n",
        "\n",
        "\n",
        "Because of $F_{0.05,4,27}=2.73$, we conclude that there is a significant interaction between material types and temperature. Furthermore, $F_{0.05,2,27}=3.35$, so the main effects of material type and temperature are also significant. Table 5.5 also shows the P-values for the test statistics.\n",
        "\n",
        "To assist in interpreting the results of this experiment, it is helpful to construct a graph of the average responses at each treatment combination. This graph is shown in Figure 5.9.\n",
        "\n",
        "![alt text](https://i.ibb.co/zGYqG5t/ex3.png)\n",
        "\n",
        "The significant interaction is indicated by the lack of parallelism of the lines. In general, longer life is attained at low temperatures, regardless of material type. Changing from low to an intermediate temperature, battery life with\n",
        "material type 3 may increase, whereas it decreases for types 1 and 2. From intermediate to high temperature, battery life decreases for material types 2 and 3 and is essentially unchanged for type 1. Material type 3 seems to\n",
        "give the best results if we want less loss of effective life as the temperature changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHtLlbIOsByg",
        "colab_type": "text"
      },
      "source": [
        "**Multiple comparisons**: When the ANOVA indicates that row or column means differ, it is usually of interest to make comparisons between the individual row or column means to discover the specific differences. The multiple comparison methods discussed in Chapter 3 are useful in this regard.\n",
        "\n",
        "We now illustrate the use of Tukey’s test on the battery life data in the previous example. Note that in this experiment, interaction is significant. When the interaction is significant, comparisons between the means of one factor (e.g., A) may be obscured by the AB interaction. One approach to this situation is to fix factor B at a specific level and apply Tukey’s test to the means of factor A at that level. To illustrate, suppose that in the example we are interested in detecting differences among the means of the three material types. Because interaction is significant, we make this comparison at just one level of temperature, say level 2 (70°F).\n",
        "\n",
        "We assume that the best estimate of the error variance is the $MS_E$ from the ANOVA table, utilizing the assumption that the experimental error variance is the same over all treatment combinations.\n",
        "\n",
        "The three material type averages at 70°F arranged in ascending order are\n",
        "\n",
        "$$\\overline{y}_{12.}=57.25 \\mspace{36mu} \\text{ (material type 1)}$$\n",
        "$$\\overline{y}_{22.}=119.75 \\mspace{36mu} \\text{(material type 2)}$$\n",
        "$$\\overline{y}_{32.}=145.75 \\mspace{36mu} \\text{(material type 3)}$$\n",
        "\n",
        "Remember that Tukey’s test declares two means significantly different if \n",
        "\n",
        "$$|\\overline{y}_{i.}-\\overline{y}_{j.}|>T_{\\alpha}=q_{\\alpha}(a,f)\\sqrt{\\frac{MS_E}{n}}$$\n",
        "\n",
        "where $f$ is the number of degrees of freedom associated with the $MS_E$ and the distribution of $q_{\\alpha}$ appears in many textbooks on statistics.\n",
        "\n",
        "In the example\n",
        "$$T_{0.05}=q_{0.05}(3,27)\\sqrt{\\frac{MS_E}{n}}=3.50\\sqrt{\\frac{675.21}{4}}=45.47$$\n",
        "\n",
        "The pairwise comparisons are\n",
        "$$|\\overline{y}_{32.}-\\overline{y}_{12.}|=|145.75-57.25|=88.5>T_{0.05}=45.47$$\n",
        "$$|\\overline{y}_{32.}-\\overline{y}_{22.}|=|145.75-119.75|=26.00< T_{0.05}=45.47$$\n",
        "$$|\\overline{y}_{22.}-\\overline{y}_{12.}|=|119.75-57.25|=62.50>T_{0.05}=45.47$$\n",
        "\n",
        "This analysis indicates that at the temperature level 70°F, the mean battery life is the same for material types 2 and 3, and that the mean battery life for material type 1 is significantly lower in comparison to both types 2 and 3.\n",
        "\n",
        "If the interaction is significant, the experimenter could compare all ab cell means to determine which ones differ significantly. In this analysis, differences between cell means include interaction effects as well as both main effects. In Example 5.1, this would give 36 comparisons between all possible pairs of the nine cell means.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9O5yAQZCIM1",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.4 Checking asssumptions of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA3MpU1CCIPm",
        "colab_type": "text"
      },
      "source": [
        "Before the conclusions from the ANOVA are adopted, the adequacy of the underlying model should be checked. As before, the primary diagnostic tool is **residual analysis**. The residuals for the two-factor factorial model with interaction are\n",
        "\n",
        "$$\\epsilon_{ijk}=y_{ijk}-\\hat{y}_{ijk}=y_{ijk}-\\overline{y}_{ij.}$$\n",
        "because the fitted value $\\hat{y}_{ijk} = \\overline{y}_{ij.}$ (the average of the observations in the $ij$th cell).\n",
        "\n",
        "The residuals from the battery life data in the example are shown in Figure 5.10. \n",
        "![alt text](https://i.ibb.co/2PPdhFD/rr.png)\n",
        "\n",
        "The normal probability plot of the residuals (Figure 5.11) does not reveal anything particularly troublesome, although the largest negative residual (-60.75 at 15°F for material type 1) does stand out somewhat from the others. The standardized value of this residual is $\\frac{-60.75}{\\sqrt{675.21}}=2.34$, and this is the only residual whose absolute value is larger than 2.\n",
        "\n",
        "Figure 5.12 plots the residuals versus the fitted values $\\hat{y}_{ijk}$. There is some mild tendency for the variance of the residuals to increase as the battery life increases. \n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://i.ibb.co/tK2NPJP/res1.png)\n",
        "\n",
        "Figures 5.13 and 5.14 plot the residuals versus material types and temperature, respectively. Both plots indicate mild inequality of variance, with the treatment combination of 15°F and material type 1 possibly having larger variance than the others.\n",
        "\n",
        "![alt text](https://i.ibb.co/m9KRdyz/res2.png)\n",
        "\n",
        "From Table 5.6 we see that the 15°F-material type 1 cell contains both extreme residuals (-60.75 and 45.25). These two residuals are primarily responsible for the inequality of variance detected in Figures 5.12, 5.13, and 5.14. Reexamination of the data does not reveal any obvious problem, such as an error in recording, so we accept these responses as legitimate. It is possible that this particular treatment combination produces a slightly more erratic battery life than the others. The problem, however, is not severe enough to have a dramatic impact on the analysis and conclusions.\n",
        "\n",
        "![alt text](https://i.ibb.co/k9Ky6xx/residual-ex.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdW-2VsSCSk7",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.5 Estimating the model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw_nfiNyCYFL",
        "colab_type": "text"
      },
      "source": [
        "The parameters in the effects model for two-factor factorial\n",
        "\n",
        "$$y_{ijk} = \\mu+\\tau_i + \\beta_j+ (\\tau\\beta)_{ij} +\\epsilon_{ijk}$$\n",
        "\n",
        "maybe estimated by least squares. Because the model has $1+a+b+ab$ parameters to be estimated, there are $1+a+b+ab$ normal equations.\n",
        "\n",
        "In order to obtain a solution, we impose the constraints\n",
        "\n",
        "$$\\sum_{i=1}^{a}\\hat{\\tau}_i=0 \\mspace{36mu} \\sum_{i=1}^{b}\\hat{\\beta}_j=0$$\n",
        "\n",
        "$$\\sum_{i=1}^{a}(\\hat{\\tau\\beta})_{ij}=0 \\mspace{36mu} j=1,...,b$$\n",
        "$$\\sum_{j=1}^{b}(\\hat{\\tau\\beta})_{ij}=0 \\mspace{36mu} i=1,...,a$$\n",
        "\n",
        "Applying these constraints, the normal equations simplify considerably, and we obtain the solution\n",
        "\n",
        "$$\\hat{\\mu}=\\overline{y}_{...}$$\n",
        "$$\\hat{\\tau_i}=\\overline{y}_{i..}-\\overline{y}_{...} \\mspace{36mu} i=1,...,a$$ \n",
        "$$\\hat{\\beta_j}=\\overline{y}_{.j.}-\\overline{y}_{...} \\mspace{36mu} j=1,...,b$$\n",
        "$$(\\hat{\\tau\\beta})_{ij}=\\overline{y}_{ij.}-\\overline{y}_{i..}-\\overline{y}_{.j.}+\\overline{y}_{...} \\mspace{36mu} i=1,...,a \\mspace{36mu} j=1,...,b$$\n",
        "\n",
        "we may find the **fitted value** $y_{ijk}$ as\n",
        "$$\\hat{y}_{ijk}=\\hat{\\mu}+\\hat{\\tau_i}+\\hat{\\beta_j}+(\\hat{\\tau\\beta})_{ij}=\\overline{y}_{ij.}$$\n",
        "\n",
        "That is, the $k$th observation in the $ij$th cell is estimated by the average of the $n$ observations in that cell. This result was used to obtain the residuals for the two-factor factorial model.\n",
        "\n",
        "Because some constraints have been used to solve the normal equations, the\n",
        "model parameters are not uniquely estimated. However, certain important **functions** of the model parameters are estimable, that is, uniquely estimated regardless of the constraint chosen. An example is $\\tau_i-\\tau_u+(\\overline{\\tau\\beta})_{i.}-(\\overline{\\tau\\beta})_{u.}$, which might be thought of as the “true” difference between the $i$th and the $u$th levels of factor A. Notice that the true difference between the levels of any main effect includes an “average” interaction effect. It is this result that disturbs\n",
        "the tests on main effects in the presence of interaction, as noted earlier. In general, any function of the model parameters that is a linear combination of the left-hand side of the normal equations is estimable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaXaE4wNCcV7",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.6 The Assumption of No Interaction in a Two-Factor Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlIIXYRnCf0U",
        "colab_type": "text"
      },
      "source": [
        "Occasionally, an experimenter feels that a two-factor model without interaction is appropriate, say\n",
        "\n",
        "$$y_{ijk} = \\mu+\\tau_i + \\beta_j +\\epsilon_{ijk} \\mspace{36mu} i=1,...,a \\mspace{12mu} j=1,...,b \\mspace{12mu} k=1,2,...,n$$ \n",
        "\n",
        "We should be very careful in dispensing with the interaction terms, however, because the presence of significant interaction can have a dramatic impact on the interpretation of the data.\n",
        "\n",
        "The statistical analysis of a two-factor factorial model without interaction is straightforward.\n",
        "\n",
        "Table 5.8 presents the analysis of the battery life data from Example 5.1, assuming that the no-interaction model applies.\n",
        "\n",
        "![alt text](https://i.ibb.co/tZ4R5wc/sss.png)\n",
        "\n",
        "\n",
        "As noted previously, both main effects are significant. However, as soon as a residual analysis is performed for these data, it becomes clear that the no-interaction model is inadequate. For the two-factor model without interaction, the fitted values are $\\hat{y}_{ijk}=\\overline{y}_{i..}+\\overline{y}_{.j.}-\\overline{y}_{...}$. A plot of $\\overline{y}_{ij.}-\\hat{y}_{ijk}$ (the cell averages minus the fitted value for that cell) versus the fitted value $\\hat{y}_{ijk}$ is shown in Figure 5.15.\n",
        "\n",
        "![alt text](https://i.ibb.co/ZSRV46h/ffff.png)\n",
        "\n",
        "Now the quantities $\\overline{y}_{ij.}-\\hat{y}_{ijk}$ may be viewed as the differences between the observed cell means and the estimated cell means assuming no interaction. Any pattern in these quantities is suggestive of the\n",
        "presence of interaction. Figure 5.15 shows a distinct pattern as the quantities $\\overline{y}_{ij.}-\\hat{y}_{ijk}$ move from positive to negative to positive to negative again. This structure is the result of the interaction between material types and temperature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99JlFpmxMJiW",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.7 One observation per cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu4m32qzMJzb",
        "colab_type": "text"
      },
      "source": [
        "Occasionally, one encounters a two-factor experiment with only a single replicate, that is, only one observation per cell. If there are two factors and only one observation per cell, the effects model is\n",
        "\n",
        "$$y_{ij} = \\mu+\\tau_i + \\beta_j+ (\\tau\\beta)_{ij} +\\epsilon_{ij} \\mspace{36mu} i=1,...,a \\mspace{12mu} j=1,...,b \\mspace{12mu}$$ \n",
        "\n",
        "The analysis of variance for this situation is shown in Table 5.9, assuming that both factors are fixed.\n",
        "\n",
        "![alt text](https://i.ibb.co/PxSPryB/1-rep.png)\n",
        "\n",
        "From examining the expected mean squares, we see that the error variance $\\sigma^2$ is not *estimable*; that is, the two-factor interaction effect $(\\tau\\beta)_{ij}$ and the experimental error cannot be separated in any obvious manner. Consequently, there are no tests on the main effects unless the interaction effect is zero. If there is no interaction present, then $(\\tau\\beta)_{ij}=0$ for all $i$ and $j$, and a plausible model is\n",
        "\n",
        "$$y_{ij} = \\mu+\\tau_i + \\beta_j +\\epsilon_{ij} \\mspace{36mu} i=1,...,a \\mspace{12mu} j=1,...,b \\mspace{12mu}$$ \n",
        "\n",
        "If the model of this equation is appropriate, then the residual mean square in Table 5.9 is an unbiased estimator of $\\sigma^2$, and the main effects may be tested by comparing $MS_A$ and $MS_B$ to $MS_{Residual}$.\n",
        "\n",
        "A test developed by Tukey (1949) helps determine whether an interaction is present. The procedure assumes that the interaction term is of a particularly simple form, namely,\n",
        "$$(\\tau\\beta)_{ij}=\\gamma\\tau_i\\beta_j$$\n",
        "\n",
        "where $\\gamma$ is an unknown constant. By defining the interaction term this way, we may use a regression approach to test the significance of the interaction term. The test partitions the residual sum of squares into a single-degree-of-freedom component due to nonadditivity (interaction) and a component for error with $(a-1)(b-1)-1$ degrees of freedom.\n",
        "Computationally, we have\n",
        "$$SS_N=\\frac{[\\sum_{i=1}^{a}\\sum_{j=1}^{b}y_{ij}y_{i.}y_{.j}-y_{..}(SS_A+SS_B+\\frac{y_{..}^2}{ab})]^2}{abSS_ASS_B}$$\n",
        "\n",
        "with one degree of freedom, and\n",
        "\n",
        "$$SS_{Error}=SS_{Residual}-SS_N$$\n",
        "\n",
        "with $(a-1)(b-1)-1$ degrees of freedom. To test for the presence of interaction, we compute\n",
        "\n",
        "$$F_0=\\frac{SS_N}{\\frac{SS_{Error}}{(a-1)(b-1)-1}}$$\n",
        "\n",
        "\n",
        "If $F_0>F_{\\alpha,1,(a-1)(b-1)-1}$ the hypothesis of no interaction must be rejected.\n",
        "\n",
        "In concluding this section, we note that the two-factor factorial model with one observation per cell looks exactly like the randomized complete block model\n",
        "viewed in chapter 4. The Tukey single-degree-of-freedom test for nonadditivity can be directly applied to test for interaction in the randomized block model. However, remember that the **experimental situations** that lead to the randomized block and factorial models are very different. In the factorial model, all $ab$ runs have been made in random order, whereas\n",
        "in the randomized block model, randomization occurs only within the block. The blocks are a randomization restriction. Hence, how the experiments are run and the interpretation of the two models are quite different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnfzrHLcYtyf",
        "colab_type": "text"
      },
      "source": [
        "## 5.4 The general factorial design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV-D3XLqYt08",
        "colab_type": "text"
      },
      "source": [
        "The results for the two-factor factorial design may be extended to the general case where there are $a$ levels of factor A, $b$ levels of factor B, $c$ levels of factor C, and so on, arranged in a factorial experiment. In general, there will be $abc...n$ total observations if there are $n$ replicates\n",
        "of the complete experiment. Once again, note that we must have at least two replicates ($n\\geq2$) to determine a sum of squares due to error if all possible interactions are included in the model.\n",
        "\n",
        "If all factors in the experiment are fixed, we may easily formulate and test hypotheses about the main effects and interactions using the ANOVA. For a fixed-effects model, test statistics for each main effect and interaction may be constructed by dividing the corresponding mean square for the effect or interaction by the mean square error. All of these $F$ tests will be\n",
        "upper-tail, one-tail tests. The number of degrees of freedom for any main effect is the number of levels of the factor minus one, and the number of degrees of freedom for interaction is the product of the number of degrees of freedom associated with the individual components of the interaction.\n",
        "\n",
        "For example, consider the **three-factor analysis of variance model**:\n",
        "\n",
        "$$y_{ijkl} = \\mu+\\tau_i + \\beta_j+ \\gamma_k+ (\\tau\\beta)_{ij} +(\\tau\\gamma)_{ik}+(\\beta\\gamma)_{jk}+(\\tau\\beta\\gamma)_{ijk}+\\epsilon_{ijkl} \\mspace{36mu} i=1,...,a \\mspace{12mu} j=1,...,b \\mspace{12mu} k=1,...,c \\mspace{12mu} l=1,...,n$$ \n",
        "\n",
        "Assuming that $A$, $B$, and $C$ are fixed, the analysis of variance table is shown in Table 5.12.\n",
        "\n",
        "The $F$ tests on main effects and interactions follow directly from the expected mean squares.\n",
        "\n",
        "Usually, the analysis of variance computations would be done using a statistics software package.\n",
        "\n",
        "![alt text](https://i.ibb.co/Q8ytYdf/table-512.png)\n"
      ]
    }
  ]
}