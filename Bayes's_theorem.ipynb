{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayes's theorem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP0bG2J3Mfd/Ahyu/uPv5fD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Statistics-and-probability/blob/master/Bayes's_theorem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NATXPHp2G3Dn"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0cwG-YIGJcH"
      },
      "source": [
        "The fundamental idea behind all Bayesian statistics is Bayes’s theorem, which is surprisingly easy to derive, provided that we understand conditional probability. So we’ll start with probability, then the conditional probability, and then Bayes’s theorem.\n",
        "\n",
        "A probability is a number between 0 and 1 (including both) that represents a degree of belief in a fact or a prediction. The value 1 represents certainty that a fact is true, or that\n",
        "a prediction will come true. The value 0 represents certainty that the fact is false. Intermediate values represent degrees of certainty. The value 0.5, often written as 50%, means that a predicted outcome is as likely to happen as not. For example, the probability that a tossed coin lands face up is very close to 50%.\n",
        "\n",
        "A conditional probability is a probability based on some background information. For example, I want to know the probability that I will have a heart attack in the next year.\n",
        "According to the [CDC](https://www.cdc.gov/heartdisease/facts.htm), \"Every year about 785,000 Americans have a first coronary\".\n",
        "The U.S. population is about 311 million, so the probability that a randomly chosen American will have a heart attack in the next year is roughly 0.3%.\n",
        " \n",
        "But I am not a randomly chosen American. Epidemiologists have identified many factors that affect the risk of heart attacks; depending on those factors, my risk might be higher or lower than average.\n",
        "\n",
        "I am male, 45 years old, and I have borderline high cholesterol. Those factors increase my chances. However, I have low blood pressure and I don’t smoke, and those factors decrease my chances.\n",
        "That value is a conditional probability because it is based on several factors that make up my \"condition\".\n",
        "\n",
        "The usual notation for conditional probability is $p(A|B), which is the probability of A given that B is true. In this example, A represents the prediction that I will have a heart attack in the next year, and B is the set of conditions we listed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKCRDmxyH1pa"
      },
      "source": [
        "## Conjoint probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QReA7uCnH3pe"
      },
      "source": [
        "**Conjoint probability** is a fancy way to say the probability that two things are true. We write $p(A\\text{ and }B)$ to mean the probability that A and B are both true.\n",
        "If we learned about probability in the context of coin tosses and dice, we might have learned the following formula:\n",
        "\n",
        "$$p(A\\text{ and }B)=p(A)p(B) \\text{ if } A \\text{ and } B \\text{ are independent}.$$\n",
        "\n",
        "For example, if we toss two coins, and A means the first coin lands face up, and B means the second coin lands face up, then $p(A)=p(B)= 0.5$, and sure enough,\n",
        "$p(A\\text{ and }B)=p(A)p(B) =0.25$\n",
        "\n",
        "\n",
        "But this formula only works because in this case A and B are independent; that is, knowing the outcome of the first event does not change the probability of the second. Or, more formally, $p(B|A)=p(B)$.\n",
        "\n",
        "Here is a different example where the events are not independent. Suppose that A means that it rains today and B means that it rains tomorrow. If I know that it rained today, it\n",
        "is more likely that it will rain tomorrow, so $p(B|A)>p(B)$.\n",
        "\n",
        "In general, the probability of a conjunction is\n",
        "$$p(A\\text{ and }B)=p(A)p(B|A)$$\n",
        "\n",
        "for any A and B. So if the chance of rain on any given day is 0.5, the chance of rain on two consecutive days is not 0.25, but probably a bit higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oTEPUtX4WDY"
      },
      "source": [
        "# Bayes theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmb18t150SAu"
      },
      "source": [
        "### Cookie problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRSFjc-O0UcM"
      },
      "source": [
        "Suppose there are two bowls of cookies. Bowl 1 contains 30 vanilla\n",
        "cookies and 10 chocolate cookies. Bowl 2 contains 20 of each.\n",
        "Now suppose we choose one of the bowls at random and, without looking, select a cookie at random. The cookie is vanilla. What is the probability that it came from Bowl1?\n",
        "\n",
        "This is a conditional probability; we want $p(\\text{bowl}_1|\\text{vanilla})$ , but it is not obvious how to compute it. If we asked a different question, the probability of a vanilla cookie given Bowl 1, it would be easy:\n",
        "\n",
        "$$p(\\text{vanilla}|\\text{bowl}_1)=\\frac{3}{4}$$\n",
        "\n",
        "Sadly, $p(A|B)$ is not the same as $p(B|A)$, but there is a way to get from one to the other: Bayes’s theorem.\n",
        "\n",
        "$$p(A|B)=\\frac{p(A)p(B|A)}{p(B)}$$\n",
        "\n",
        "We can use it to solve the cookie problem. I’ll write $B_1$ for the hypothesis that the cookie came from Bowl 1 and $V$ for the vanilla cookie. Plugging in Bayes’s theorem we get\n",
        "\n",
        "$$p(B_1|V)=\\frac{p(B_1)p(V|B_1)}{p(V)}=\\frac{(\\frac{1}{2})(\\frac{3}{4})}{\\frac{5}{8}}=\\frac{3}{5}$$\n",
        "\n",
        "- $p(B_1)$ : This is the probability that we chose Bowl 1, unconditioned by what kind of cookie we got. Since the problem says we chose a bowl at random, we can assume $p(B_1)=\\frac{1}{2}$.\n",
        "\n",
        "- $p(V|B_1)$: This is the probability of getting a vanilla cookie from Bowl 1, which is $\\frac{3}{4}$.\n",
        "\n",
        "- $p(V)$ : This is the probability of drawing a vanilla cookie from either bowl. Since we had an equal chance of choosing either bowl and the bowls contain the same number of cookies, we had the same chance of choosing any cookie. Between the two bowls, there are 50 vanilla and 30 chocolate cookies, so $p(V)=\\frac{5}{8}$.\n",
        "\n",
        "So the vanilla cookie is evidence in favor of the hypothesis that we chose Bowl 1, because vanilla cookies are more likely to come from Bowl 1.\n",
        "\n",
        "This example demonstrates one use of Bayes’s theorem: it provides a strategy to get from $p(B|A)$ to $p(A|B)$. This strategy is useful in cases, like the cookie problem, where it is easier to compute the terms on the right side of Bayes’s theorem than the term on the left.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_H5YHgl4dr2"
      },
      "source": [
        "There is another way to think of Bayes’s theorem: it gives us a way to update the probability of a hypothesis, H, in light of some data, D.\n",
        "\n",
        "Rewriting Bayes’s theorem with H and D yields:\n",
        "$$p(H|D)=\\frac{p(H)p(D|H)}{p(D)}$$\n",
        "In this interpretation, each term has a name:\n",
        "\n",
        "- $p(H)$ is the probability of the hypothesis before we see the data, called the *prior probability*, or just *prior*.\n",
        "\n",
        "- $p(H|D)$ is what we want to compute, the probability of the hypothesis after we see the data, called the *posterior*.\n",
        "\n",
        "- $p(D|H)$ is the probability of the data under the hypothesis, called the *likelihood*.\n",
        "\n",
        "- $p(D)$ is the probability of the data under any hypothesis, called the *normalizing constant*.\n",
        "\n",
        "Sometimes we can compute the prior based on background information. For example,\n",
        "the cookie problem specifies that we choose a bowl at random with equal probability (the prior distribution follow a [Bernoulli distribution](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_de_Bernoulli) with $p=\\frac{1}{2}$, or equivalently in this case, a [discrete uniform distribution](https://en.wikipedia.org/wiki/Discrete_uniform_distribution) on $\\{0,1\\}$). In other cases the prior is subjective; that is, reasonable people might disagree, either because they use different background information or because they interpret the same information differently.\n",
        "\n",
        "The likelihood is usually the easiest part to compute. In the cookie problem, if we know which bowl the cookie came from, we find the probability of a vanilla cookie by counting.\n",
        "\n",
        "The normalizing constant can be tricky. It is supposed to be the probability of seeing the data under any hypothesis at all, but in the most general case, it is hard to nail down what that means.\n",
        "Most often we simplify things by specifying a set of hypotheses that are:\n",
        "Mutually exclusive (At most one hypothesis in the set can be true), and\n",
        "Collectively exhaustive (at least one of the hypotheses has to be true).\n",
        "\n",
        "In the cookie problem, there are only two hypotheses, the cookie came from Bowl 1 or Bowl 2, and they are mutually exclusive and collectively exhaustive.\n",
        "In that case we can compute $p(D)$ using the law of total probability.\n",
        "\n",
        "**Note**: In statistics, a [**probability distribution**](https://en.wikipedia.org/wiki/Probability_distribution) of a random variable X is a set of values that the random variable can take and their corresponding probabilities. For example, if we roll a six-sided die, the set of possible values is the numbers 1 to 6, and the probability associated with each value is $\\frac{1}{6}$. The [*probability mass function*](https://en.wikipedia.org/wiki/Probability_mass_function) (pmf) is a way to represent a discrete distribution mathematically. In the six-sided die example, the pmf is $p(X=x)=\\frac{1}{6} \\forall x=1,2,..,6$. In the context of Bayes’s theorem, it is natural to use a pmf to map from each hypothesis to its probability. This distribution, which contains the priors for each hypothesis, is called the **prior distribution**. The distribution that contains the posterior probability for each hypothesis, is called the **posterior distribution**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tU1Q7BP632L"
      },
      "source": [
        "## The dice problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vGC306S66tr"
      },
      "source": [
        "Suppose we have a box of dice that contains a 4-sided dice, a 6-sided dice, an 8-sided dice, a 12-sided dice, and a 20-sided dice. Suppose we select a dice from the box at random, roll it, and get a 6. What is the probability that we rolled each dice?\n",
        "Let's apply the Bayes theorem to solve this problem. We follow a three-step strategy for approaching a problem like this.\n",
        "1. We choose a representation for the hypotheses. $H_4$, $H_6$, $H_8$, $H_{12}$, and $H_{20}$ represent the hypotheses that we roll the 4-sided dice, the 6-sided dice, ...\n",
        "\n",
        "2. We choose a representation for the data (we got a 6 between for a range from 1 to 20), D.\n",
        "\n",
        "3. We write the likelihood function $p(D|H)$. What is the probability of getting a six if we roll the 4-sided dice? Obviously, $p(D|H_4)=0$, and if we roll the 6-sided dice?, $p(D|H_6)=\\frac{1}{6}$, and similarly we get $p(D|H_8)=\\frac{1}{8}$, $p(D|H_{12})=\\frac{1}{12}$ and $p(D|H_{20})=\\frac{1}{20}$. \n",
        "\n",
        "We can assume that the probability of the hypothesis before we see the data (the prior) is $p(H_i)=\\frac{1}{5}$ for each hypothesis $H_i$ ($i=4,6,8,12,20$), since the problem says we chose a dice randomly, and there are five dices (the prior distribution follow a discrete uniform distribution on $\\{0,1,2,3,4\\}$).\n",
        "\n",
        "Lastly, the probability of the data under any hypothesis (normalizing constant), $p(D)$ can be computed easily using the law of total probability since the hypotheses are mutually exclusive and collectively exhaustive: \n",
        "$$p(D)=\\sum_{i=4,6,8,12,20}p(H_i)p(D|H_i)=\\frac{1}{5}\\sum_{i=4,6,8,12,20}p(D|H_i)=\\frac{51}{600}$$\n",
        "\n",
        "Now we can compute the posterior for each hypotheses easily using the bayes theorem. For example, the probability that we rolled the 6-sided dice is:\n",
        "\n",
        "$$p(H_6|D)=\\frac{p(H_6)p(D|H_6)}{p(D)}=\\frac{\\frac{1}{5}\\frac{1}{6}}{\\frac{51}{600}} \\approx 0.392$$\n",
        "\n",
        "Similarly we can compute $p(H_4|D)=0$, $p(H_8|D)=0.294$, $p(H_{12}|D)=0.196$ and $p(H_{12}|D)=0.118$.\n",
        "\n",
        "The most likely alternative is the 6-sided dice, but there is still almost a 12% chance for the 20-sided dice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRMprVYELbfa"
      },
      "source": [
        "## The locomotive problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMmqpU3rLfpY"
      },
      "source": [
        "\"A railroad numbers its locomotives in order 1..N. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad has\".\n",
        "\n",
        "Based on this observation, we know the railroad has 60 or more locomotives. But how many more? To apply Bayesian reasoning, we can break this problem into two steps:\n",
        "\n",
        "- What did we know about $N$ before we saw the data? The answer to this question is the prior.\n",
        "\n",
        "- For any given value of $N$, what is the likelihood of seeing the data (a locomotive with number 60)? The answer to this question is the likelihood.\n",
        "\n",
        "We don’t have much basis to choose a prior, but we can start with something simple and then consider alternatives. Let’s assume that $N$ is equally likely to be any value from 1 to 1000 (the prior distribution follows a discrete uniform distribution on $\\{0,1,...,999\\}$). \n",
        "\n",
        "Now, all we need is a likelihood function. In a hypothetical fleet of $N$ locomotives, what is the probability that we would see number 60? If we assume that there is only one train-operating company (or only one we care about) and that we are equally likely to see any of its locomotives, then the chance of seeing any particular locomotive is $\\frac{1}{N}$. We choose a representation for the hypotheses: $H_1$,..., $H_{1000}$ represent the hypotheses that the number of locomotives is $1,...,1000$. Obviously, $p(D|H_i)=0$ $\\forall i=1,...,59 \\Rightarrow p(H_i|D)=0$ applying the Bayes theorem. The posterior for the rest of the hypotheses computed by the Bayes theorem is shown in the next figure:\n",
        "\n",
        "![](https://i.ibb.co/mHj4MZN/posterior-dist-locomotive-problem.png)\n",
        "\n",
        "\n",
        "The most likely value is 60. That might not seem like a very good guess; after all, what are the chances that you just happened to see the train with the\n",
        "highest number? Nevertheless, if you want to maximize the chance of getting the answer exactly right, you should guess 60. But maybe that’s not the right goal. An alternative is to compute the mean of the posterior distribution. The mean of the posterior is 333, so that might be a good guess if you wanted to minimize\n",
        "error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU3gGHx0cIoE"
      },
      "source": [
        "### What about that prior?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49-PzO12cRmJ"
      },
      "source": [
        "To make any progress on the locomotive problem we had to make assumptions, and some of them were pretty arbitrary. In particular, we chose a uniform prior from 1 to 1000, without much justification for choosing 1000, or for choosing a uniform distribution. It is not crazy to believe that a railroad company might operate 1000 locomotives, but a reasonable person might guess more or fewer. So we might wonder whether the posterior distribution is sensitive to these assumptions. \n",
        "\n",
        "Recall that with a uniform prior from 1 to 1000, the mean of the posterior is 333. With an upper bound of 500, we get a posterior mean of 207, and with an upper bound of 2000, the posterior mean is 552.\n",
        "So that’s bad. There are two ways to proceed:\n",
        "- Get more data.\n",
        "- Get more background information.\n",
        "\n",
        "With more data, posterior distributions based on different priors tend to converge. For example, suppose that in addition to train 60 we also see trains 30 and 90. With these data, the means of the posteriors are 164 with an upper bound of 1000, 152 with an upper bound of 500, and 171 with an upper bound of 2000. So the differences are smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmx_asxjdKoR"
      },
      "source": [
        "#### An alternative prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szJdBkCbdNpb"
      },
      "source": [
        "If more data are not available, another option is to improve the priors by gathering more background information. It is probably not reasonable to assume that a train-operating company with 1000 locomotives is just as likely as a company with only 1.\n",
        "We could find a list of companies that operate locomotives in the area of observation. Or we could interview an expert in rail shipping to gather\n",
        "information about the typical size of companies.\n",
        "But even without getting into the specifics of railroad economics, we can make some educated guesses. In most fields, there are many small companies, fewer medium-sized companies, and only one or two very large companies. In fact, the distribution of company sizes tends to follow a power law, as Robert Axtell reports in [Science](http://www.sciencemag.org/content/293/5536/1818.full.pdf).\n",
        "\n",
        "This law suggests that if there are 1000 companies with fewer than 10 locomotives, there might be 100 companies with 100 locomotives, 10 companies with 1000, and possibly one company with 10000 locomotives.\n",
        "Mathematically, a power-law means that the number of companies with a given size is inversely proportional to size:\n",
        "\n",
        "$$P(X=x)=(\\frac{1}{x})^{\\alpha}$$\n",
        "\n",
        "where $P(X=x)$ is the probability mass function of $x$ and $\\alpha$ is a parameter that is often near 1. We can construct a power-law prior. Again, the upper bound is arbitrary, but with a power-law prior, the posterior is less\n",
        "sensitive to this choice.\n",
        "\n",
        "Figure 3-2 shows the new posterior based on the power law, compared to the posterior based on the uniform prior. Using the background information represented in the power-law prior, we can all but eliminate values of N greater than 700.\n",
        "\n",
        "![](https://i.ibb.co/6JTYBry/posterior-2.png)\n",
        "\n",
        "If we start with this prior and observe trains 30, 60, and 90, the means of the posteriors are 133 with an upper bound of 1000, 131 with an upper bound of 500, and 134 with an upper bound of 2000. Now the differences are much smaller. In fact, with an arbitrarily large upper bound, the mean converges on 134.\n",
        "\n",
        "So the power-law prior is more realistic, because it is based on general information about the size of companies, and it behaves better in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS-MDeGfR2rH"
      },
      "source": [
        "#### Credible intervals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7wHP7vcR4yz"
      },
      "source": [
        "Once you have computed a posterior distribution, it is often useful to summarize the results with a single point estimate or an interval. For point estimates it is common to use the mean, median, or the value with maximum likelihood.\n",
        "\n",
        "For intervals, we usually report two values computed so that there is a 90% chance that the unknown value falls between them (or any other probability). These values define a *credible interval*.\n",
        "\n",
        "A simple way to compute a credible interval is to add up the probabilities in the posterior distribution and record the values that correspond to probabilities 5% and 95% (that is the $5^{\\text{th}}$ and $95^{\\text{th}}$ percentiles.\n",
        "\n",
        "For the locomotive problem with a power-law prior and three trains, the 90% credible interval is $(91,243)$. The width of this range suggests, correctly,\n",
        "that we are still quite uncertain about how many locomotives there are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL_kTG73Sekk"
      },
      "source": [
        "## Odds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Uu4Yy0ShT1"
      },
      "source": [
        "One way to represent a probability is with a number between 0 and 1, but that is not the only way. If we have ever bet on a football game or a horse race, we have probably encountered another representation of probability, called **odds**.\n",
        "\n",
        "We might have heard expressions like \"the odds are three to one\". The odds in favor of an event are the ratio of the probability it will occur to the probability that it will not.\n",
        "So if we think my team has a 75% chance of winning, I would say that the odds in their favor are three to one, because the chance of winning is three times the chance of losing.\n",
        "We can write odds in decimal form, but it is most common to write them as a ratio of integers. So \"three to one\" is written 3:1.\n",
        "When probabilities are low, it is more common to report the odds against rather than the odds in favor. For example, if we think my horse has a 10% chance of winning, I would say that the odds against are 9:1.\n",
        "Probabilities and odds are different representations of the same information. Given a probability $p$, we can compute the odds:\n",
        "\n",
        "$$\\text{Odds}=\\frac{p}{1-p}$$\n",
        "\n",
        "Inversely, given the odds in favor, in decimal form, we can convert to probability:\n",
        "\n",
        "$$p=\\frac{\\text{Odds}}{\\text{Odds}+1}$$\n",
        "\n",
        "For example, if 20% of people think our horse will win, then 80% of them don’t, so the odds in favor are 20:80 or 1:4. If the odds are 5:1 against our horse, then five out of six people think she will lose, so the probability of winning is $\\frac{1}{6}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lKJvh0VTx5D"
      },
      "source": [
        "### The odds form of Bayes’s theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhlXLMfrTcZv"
      },
      "source": [
        "We have already saw how to writte the Bayes’s theorem in the **probability form**:\n",
        "\n",
        "$$p(H|D)=\\frac{p(H)p(D|H)}{p(D)}$$\n",
        "\n",
        "If we have two hypotheses, A and B, we can write the ratio of posterior probabilities like this:\n",
        "\n",
        "$$\\frac {p(A|D)}{p(B|D)}=\\frac{p(A)p(D|A)}{p(B)p(D|B)}$$\n",
        "\n",
        "Notice that the normalizing constant, $p(D)$, drops out of this equation.\n",
        "\n",
        "If A and B are mutually exclusive and collectively exhaustive, that means\n",
        "$p(B)=1−p(A)$, so we can rewrite the ratio of the priors, and the ratio of the posteriors, as odds.\n",
        "\n",
        "Writing $\\text{odds}(A)$ for odds in favor of A, we get:\n",
        "\n",
        "$$\\text{odds}(A|D)=\\frac {p(A|D)}{p(B|D)}=\\frac{p(A)p(D|A)}{p(B)p(D|B)}=\\frac{p(A)p(D|A)}{(1-p(A))p(D|B)}=\\text{odds}(A)\\frac{p(D|A)}{p(D|B)}$$\n",
        "\n",
        "In words, this says that the posterior odds are the prior odds times the likelihood ratio. This is the **odds form** of Bayes’s theorem.\n",
        "\n",
        "This form is most convenient for computing a Bayesian update on paper or in our\n",
        "head. For example, let’s go back to the cookie problem:\n",
        "\n",
        "The prior probability is 50%, so the prior odds are $1:1$, or just $1$. The likelihood ratio is $\\frac{\\frac{3}{4}}{\\frac{1}{2}}=\\frac{3}{2}$. So the posterior odds are $3:2$, which corresponds to probability $\\frac{3}{5}$.\n",
        "\n",
        "Let's propose another question for the cookie problem. Do these data (we get a vanilla cookie) give evidence in favor of the proposition that cookies come from bowl 1?\n",
        "\n",
        "To answer this question, we need to think about what it means for data to give evidence in favor of (or against) a hypothesis. Intuitively, we might say that data favor a hypothesis if the hypothesis is more likely in light of the data than it was before.\n",
        "In the cookie problem, the prior odds are 1:1, or probability 50%. The posterior odds are 3:2, or probability 60%. So we could say that the vanilla cookie is evidence in favor of Bowl 1.\n",
        "\n",
        "The odds form of Bayes’s theorem provides a way to make this intuition more precise.\n",
        "\n",
        "Again\n",
        "\n",
        "$$\\text{odds}(A|D)=\\text{odds}(A)\\frac{p(D|A)}{p(D|B)}$$\n",
        "\n",
        "Or dividing through by $\\text{odds}(A)$:\n",
        "\n",
        "$$\\frac{\\text{odds}(A|D)}{\\text{odds}(A)}=\\frac{p(D|A)}{p(D|B)}$$\n",
        "\n",
        "The term on the left is the ratio of the posterior and prior odds. The term on the right is the likelihood ratio, also called the **Bayes factor**.\n",
        "\n",
        "- If the Bayes factor value is greater than 1, that means that the data were more likely under A than under B. And since the odds ratio is also greater than 1, that means that the odds are greater, in light of the data, than they were before.\n",
        "\n",
        "- If the Bayes factor is less than 1, that means the data were less likely under A than under B, so the odds in favor of A go down.\n",
        "\n",
        "- Finally, if the Bayes factor is exactly 1, the data are equally likely under either hypothesis, so the odds do not change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj46YSFn6vXE"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eoy3n7GVJW5o"
      },
      "source": [
        "- Think Bayes: Bayesian Statistics in Python"
      ]
    }
  ]
}